{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqavKP0TIv6BaIN46O6sng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgqf7/AIPROJECTUMSL/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1OVwh107QrFZ",
        "outputId": "726d5df4-920c-4bea-805c-34872b8681c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3g0lEQVR4nO3dd3hUVeI+8HdmMjPpvYcUSEIMAQKIsBRpUkSaXxcB14IormuvWHAFRFF/NkDsZUFEVGSlCEpvC9J7h5AQUkhCep1kyv39ERgJJBDSztx738/z5MFMpryJMG/OOfeeq5EkSQIREREAregARETkOFgKRERkx1IgIiI7lgIREdmxFIiIyI6lQEREdiwFIiKyYykQEZEdS4GIiOxYClSnqKgoPPjgg6JjNNq0adOg0Wha5LX69euHfv362T/ftGkTNBoNFi9e3CKv/+CDDyIqKqpFXouUiaWgQmfOnMGjjz6KNm3awNnZGZ6enujVqxdmz56NiooK0fGuad68edBoNPYPZ2dnhIaGYsiQIfj4449RUlLSJK+TmZmJadOm4cCBA03yfE3JkbOR/DmJDkAta+XKlbj77rthNBrxwAMPoH379qiqqsLWrVsxadIkHD16FF999ZXomNc1ffp0tG7dGmazGVlZWdi0aROeffZZfPTRR1i+fDk6duxov++///1vvPLKKzf0/JmZmXjjjTcQFRWFTp061ftxa9asuaHXaYhrZfv6669hs9maPQMpF0tBRVJSUjBu3DhERkZiw4YNCAkJsX/tiSeeQFJSElauXCkwYf0NHToUXbt2tX/+6quvYsOGDRg+fDhGjhyJ48ePw8XFBQDg5OQEJ6fm/ateXl4OV1dXGAyGZn2d69Hr9UJfn+SP00cq8t5776G0tBTffvttjUK4JCYmBs8880ydj8/Pz8eLL76IDh06wN3dHZ6enhg6dCgOHjx41X3nzJmDhIQEuLq6wsfHB127dsXChQvtXy8pKcGzzz6LqKgoGI1GBAYGYtCgQdi3b1+Dv78BAwbg9ddfR2pqKhYsWGC/vbY1hbVr16J3797w9vaGu7s74uLiMHnyZADV6wC33HILAGDChAn2qap58+YBqF43aN++Pfbu3Ys+ffrA1dXV/tgr1xQusVqtmDx5MoKDg+Hm5oaRI0ciLS2txn3qWsO5/Dmvl622NYWysjK88MILCA8Ph9FoRFxcHD744ANcuUGyRqPBk08+iaVLl6J9+/YwGo1ISEjAqlWrav+BkyJxpKAiv/32G9q0aYOePXs26PHJyclYunQp7r77brRu3RrZ2dn48ssv0bdvXxw7dgyhoaEAqqcwnn76aYwePRrPPPMMTCYTDh06hJ07d+If//gHAOBf//oXFi9ejCeffBLt2rVDXl4etm7diuPHj6NLly4N/h7vv/9+TJ48GWvWrMEjjzxS632OHj2K4cOHo2PHjpg+fTqMRiOSkpKwbds2AEB8fDymT5+OKVOm4J///CduvfVWAKjxc8vLy8PQoUMxbtw43HfffQgKCrpmrhkzZkCj0eDll19GTk4OZs2ahYEDB+LAgQP2EU191Cfb5SRJwsiRI7Fx40Y8/PDD6NSpE1avXo1JkyYhIyMDM2fOrHH/rVu34tdff8Xjjz8ODw8PfPzxx/j73/+Oc+fOwc/Pr945ScYkUoWioiIJgDRq1Kh6PyYyMlIaP368/XOTySRZrdYa90lJSZGMRqM0ffp0+22jRo2SEhISrvncXl5e0hNPPFHvLJfMnTtXAiDt3r37ms/duXNn++dTp06VLv+rPnPmTAmAdOHChTqfY/fu3RIAae7cuVd9rW/fvhIA6Ysvvqj1a3379rV/vnHjRgmAFBYWJhUXF9tvX7RokQRAmj17tv22K3/edT3ntbKNHz9eioyMtH++dOlSCYD01ltv1bjf6NGjJY1GIyUlJdlvAyAZDIYatx08eFACIM2ZM+eq1yJl4vSRShQXFwMAPDw8GvwcRqMRWm31Xxmr1Yq8vDz71Mvl0z7e3t5IT0/H7t2763wub29v7Ny5E5mZmQ3OUxd3d/drHoXk7e0NAFi2bFmDF2WNRiMmTJhQ7/s/8MADNX72o0ePRkhICH7//fcGvX59/f7779DpdHj66adr3P7CCy9AkiT88ccfNW4fOHAgoqOj7Z937NgRnp6eSE5Obtac5DhYCirh6ekJAI06ZNNms2HmzJmIjY2F0WiEv78/AgICcOjQIRQVFdnv9/LLL8Pd3R3dunVDbGwsnnjiCfvUzCXvvfcejhw5gvDwcHTr1g3Tpk1rsjee0tLSa5bf2LFj0atXL0ycOBFBQUEYN24cFi1adEMFERYWdkOLyrGxsTU+12g0iImJwdmzZ+v9HA2RmpqK0NDQq34e8fHx9q9fLiIi4qrn8PHxQUFBQfOFJIfCUlAJT09PhIaG4siRIw1+jrfffhvPP/88+vTpgwULFmD16tVYu3YtEhISaryhxsfH4+TJk/jpp5/Qu3dv/Pe//0Xv3r0xdepU+33GjBmD5ORkzJkzB6GhoXj//feRkJBw1W+uNyo9PR1FRUWIiYmp8z4uLi7YsmUL1q1bh/vvvx+HDh3C2LFjMWjQIFit1nq9zo2sA9RXXSfY1TdTU9DpdLXeLvGqvarBUlCR4cOH48yZM9i+fXuDHr948WL0798f3377LcaNG4fBgwdj4MCBKCwsvOq+bm5uGDt2LObOnYtz585h2LBhmDFjBkwmk/0+ISEhePzxx7F06VKkpKTAz88PM2bMaOi3BwD4/vvvAQBDhgy55v20Wi1uu+02fPTRRzh27BhmzJiBDRs2YOPGjQDqfoNuqNOnT9f4XJIkJCUl1ThSyMfHp9af5ZW/zd9ItsjISGRmZl41Qjxx4oT960SXYymoyEsvvQQ3NzdMnDgR2dnZV339zJkzmD17dp2P1+l0V/3G+MsvvyAjI6PGbXl5eTU+NxgMaNeuHSRJgtlshtVqrTHdBACBgYEIDQ1FZWXljX5bdhs2bMCbb76J1q1b4957763zfvn5+VfddukksEuv7+bmBgC1vkk3xPz582u8MS9evBjnz5/H0KFD7bdFR0djx44dqKqqst+2YsWKqw5dvZFsd9xxB6xWKz755JMat8+cORMajabG6xMBPCRVVaKjo7Fw4UKMHTsW8fHxNc5o/vPPP/HLL79cc6+j4cOHY/r06ZgwYQJ69uyJw4cP44cffkCbNm1q3G/w4MEIDg5Gr169EBQUhOPHj+OTTz7BsGHD4OHhgcLCQrRq1QqjR49GYmIi3N3dsW7dOuzevRsffvhhvb6XP/74AydOnIDFYkF2djY2bNiAtWvXIjIyEsuXL4ezs3Odj50+fTq2bNmCYcOGITIyEjk5Ofjss8/QqlUr9O7d2/6z8vb2xhdffAEPDw+4ubmhe/fuaN26db3yXcnX1xe9e/fGhAkTkJ2djVmzZiEmJqbGYbMTJ07E4sWLcfvtt2PMmDE4c+YMFixYUGPh90azjRgxAv3798drr72Gs2fPIjExEWvWrMGyZcvw7LPPXvXcRDwkVYVOnTolPfLII1JUVJRkMBgkDw8PqVevXtKcOXMkk8lkv19th6S+8MILUkhIiOTi4iL16tVL2r59+1WHTH755ZdSnz59JD8/P8loNErR0dHSpEmTpKKiIkmSJKmyslKaNGmSlJiYKHl4eEhubm5SYmKi9Nlnn103+6VDUi99GAwGKTg4WBo0aJA0e/bsGod9XnLlIanr16+XRo0aJYWGhkoGg0EKDQ2V7rnnHunUqVM1Hrds2TKpXbt2kpOTU41DQPv27VvnIbd1HZL6448/Sq+++qoUGBgoubi4SMOGDZNSU1OvevyHH34ohYWFSUajUerVq5e0Z8+eq57zWtmuPCRVkiSppKREeu6556TQ0FBJr9dLsbGx0vvvvy/ZbLYa9wNQ62HCdR0qS8qkkSSuIBERUTWuKRARkR1LgYiI7FgKRERkx1IgIiI7lgIREdmxFIiIyI6lQEREdiwFIiKyYykQEZEdS4GIiOxYCkREZMdSICIiO5YCERHZsRSIiMiOpUBERHYsBSIismMpEBGRHUuBiIjsWApERGTHUiAiIjuWAhER2bEUiIjIjqVARER2LAUiIrJjKRARkR1LgYiI7FgKRERkx1IgIiI7lgIREdmxFIiIyI6lQEREdiwFIiKyYykQEZEdS4GIiOxYCkREZMdSULlPP/0UUVFRcHZ2Rvfu3bFr1y7RkYhIIJaCiv388894/vnnMXXqVOzbtw+JiYkYMmQIcnJyREcjIkE0kiRJokOQGN27d8ctt9yCTz75BABgs9kQHh6Op556Cq+88orgdEQkAkcKKlVVVYW9e/di4MCB9tu0Wi0GDhyI7du3C0xGRCKxFFQqNzcXVqsVQUFBNW4PCgpCVlaWoFREJBpLgYiI7FgKKuXv7w+dTofs7Owat2dnZyM4OFhQKiISjaWgUgaDATfffDPWr19vv81ms2H9+vXo0aOHwGREJJKT6AAkzvPPP4/x48eja9eu6NatG2bNmoWysjJMmDBBdLRGsdokFJRXoaCsCvllVdX/XW5GQXkVKqqssNgkWKw2WGwSrDap+k+rBLPNBptNgpNOC1eDDq4Gp4t/1vxvN6MTvFz0CPQ0wt/NCK1WI/pbJmoyLAUVGzt2LC5cuIApU6YgKysLnTp1wqpVq65afHY0JSYzUvPKcTavDKl55Ui9+Gd2sQn5ZVUoqbSgpQ601mk18HUzIMjTiBAvF4R5uyDEyxlhPi6I8nNDTKA7nPW6lglD1AR4ngI5rKJyMw5lFOJwRhFOZ5faSyC/rEp0tHrTaoAIX1e0DfJA2yAPxAa5o22QB9oEuMHoxLIgx8NSIIdQVGHG0YwiHMoowuH0IhzOKMK5/HLRsZqNTqtBlJ8rElt5o2uUL7pG+SA20B0aDaeiSCyWAgmRW1qJbUm52JaUi91nC3A2r6zFpnwclberHl0ifNA1ygddI33RsZUXp56oxbEUqEVUVFmxMyUPW0/nYmtSLk5ml6i+BK7HoNOiYysv9G0bgAHxgUgI9RIdiVSApUDNJimnFKuPZmHLqQvYf64QVVab6EiyFurljP43BeK2+ED0jPbnKIKaBUuBmlRSTil+P3weKw+dx8nsEtFxFMtFr0PPaD8MiA/EoHZBCPRwFh2JFIKlQI3GIhBLp9WgV4w/7uochiEJwXAxcARBDcdSoAbJKTZh8b50LNufySJwIO5GJwxJCMZdXcLQo40fT6yjG8ZSoHqz2iRsPJGDn3anYdPJHFhs/KvjyEK8nDGqUxhG3xyGmEAP0XFIJlgKdF05xSb8uCsNP+0+h/NFJtFxqAF6tPHDhF5RGBgfxNEDXRNLgeq0MzkP87enYs2xLJit/GuiBBG+rhjfMwpjuraCh7NedBxyQCwFqkGSJKw7noPPNiVh/7lC0XGombgbnTD65laY0CsKkX5uouOQA2EpEIDq9YLfDmbi801nuHCsIloNMOCmQDzePwZdInxExyEHwFJQuUqLFYv3puPLzcmK3muIrq9/XACeHxSHDq145rSasRRUymS24vvtqfj6f8nIKakUHYccyKB2QXhuYFu0C/UUHYUEYCmojCRJ+HVfBj5ccxKZPJKI6qDRAEPbB+O5gW0RG8TDWdWEpaAifyblYsbvx3E0s1h0FJIJrQYY3jEUk4bEIdzXVXQcagEsBRU4lV2Cd34/jo0nL4iOQjJldNLi0T5t8Hj/GG7Ep3AsBQXLKTFh5tpTWLQnHVaefUxNIMzbBa8Ni8cdHUJER6FmwlJQIJtNwvc7UvH+6pMorbSIjkMK1CvGD9NGJHC9QYFYCgpzMqsEr/x6iCeeUbNz0mowvmcUnh0Yy7OjFYSloBAmsxVzNpzGV1uSuSUFtSh/dyPeHJWAoZxSUgSWggL8eSYXry05gpTcMtFRSMWGdQjB9FEJ8HM3io5CjcBSkLGicjPeWnkMv+xNFx2FCADg62bAGyMTMCIxVHQUaiCWgkztTM7Dsz8f4FbW5JBGJIbirVHt4eXKtQa5YSnIjNUmYfa6U/hkYxJ4lCk5siBPI94bnYi+bQNER6EbwFKQkfSCcjzz0wHsTS0QHYWo3ib0isLkO+Kh12lFR6F6YCnIxIpDmZj862EUm3jeAclPlwhvfHpvF4R4uYiOQtfBUnBw5VUWTFt+FIv2cDGZ5M3PzYDZ4zqjd6y/6Ch0DSwFB5aaV4aJ3+3B6ZxS0VGImoRWAzw7sC2eGhADjYbXinZELAUHtS0pF08s3IfCcrPoKERNrm/bAMwa2wk+bgbRUegKLAUHNG9bCt5aeRwWHl5EChbm7YLP7u2CxHBv0VHoMiwFB2K22jBl2RH8uCtNdBSiFuGs12LW2M64vX2w6Ch0EUvBQeSVVuKxBfuw62y+6ChELUqrAV4b1g4P924tOgqBpeAQjp8vxsTv9iCjsEJ0FCJhHuwZhSnD20Gr5QK0SCwFwXYm52Hid3tQwuseEGFQuyB8PK4zXAy8upsoLAWB1h/PxhML98FktomOQuQwEsO98e34rvDnbqtCsBQEWbo/Ay/+cpBHGBHVItzXBd9N6IY2Ae6io6gOS0GAedtS8MaKY+BPnqhuAR5G/PhId8QE8pKfLYml0MJmrTuFWetOi45BJAv+7gYsfORvaMtrQbcYlkILkSQJ01ccw9xtZ0VHIZIVPzcDFkzsjvgQT9FRVIGl0EKmLDuC+dtTRccgkiUfVz0WTOyOhFAv0VEUjxuct4B3/jjOQiBqhIJyM/7x9U4cTi8SHUXxWArN7OP1p/Hl5mTRMYhkr6jCjHu/2YGDaYWioygaS6EZffO/ZHy09pToGESKUWyy4P5vd+JkVonoKIrFUmgmP+46h7dWHhcdg0hxik0WPDh3FzK5LUyzYCk0g6X7M/DaksOiYxAp1vkiEx74zy4UlleJjqI4LIUmtu5YNl785SB4ojJR80rKKcXD3+2ByWwVHUVRWApN6FB6IZ76cT+3riBqIXtTC/Dkwv2w8t9ck2EpNJGMwgo8/N0eVPC3FqIWte54Nv69lNO1TYWl0ARKTGY8NHc3LpRUio5CpEo/7krDTB7p1yRYCo1ks0l46sf9OJnNQ+SIRJq9/jT+OHxedAzZYyk00jt/HMemkxdExyAiAC/+chCn+Qtao7AUGmHx3nR8/b8U0TGI6KKyKise/X4vik1m0VFki6XQQAfSCjGZ5yIQOZzk3DI8//MBcK/PhmEpNEBRhRlPLtyHKgsvo0nkiNYdz8Hs9bxuSUOwFBrg5cWHkF7AU+yJHNns9aex7li26Biyw1K4Qd/9eRarjmaJjkFE1yFJwHOLDiAlt0x0FFlhKdyAIxlFmPE7N7kjkosSkwXP/LQfFiuneuuLV16rp9JKC0bM2crfOi6ylOSicNM8VCTvhWSphJN3CPzueBbGkFhIVgsK//c9Ks7sgaUoC1qjG5wjE+Hd90E4efjV+ZxF2xeh/NR2mPPToXEywBgWD5++D0Lv18p+n/z1X6PsyHpo9M7w7jse7gn97V8rO7EVZUfWI3D01Gb93kl+nhoQgxcGx4mOIQtOogPIxeRfD7MQLrKaSpG14CU4R3RE4N3ToHX1gqUgE1pndwCAZKlEVdYZePUcB0Nga9hMpchf/xUu/PomQsbPqvN5TWlH4NFlGAzBsYBkReHm+che9DpCH/4cWoMzypN2ouz4ZgSOeROWgkzk/TEbLq27QOfqBVtlGQq3zEfQuLda6KdAcvLZpjPoFxeImyN9REdxeJw+qoefd5/D8oOZomM4jOIdi+Hk6Q//Yc/CGBoHvXcwXFp3gd4nBACgNbohaNxbcIu/FXq/VjCG3QTfQf9CVVYSLMU5dT5v0JjpcO8wEIaASBgC28Bv2HOwFl9AVXYSAMCclwbn8A4whsTCrV1faAyusBRVLyQWbJwLj853wMkzsPl/ACQ7VpuE534+gLJKi+goDo+lcB2ZhRV4cwXXES5XkbQThuBYXFj6DtLm3IvMuU+j5MCqaz7GVlkOQAOt0b3er2OrrB6ZXRqBGAJaoyorCVZTKSqzkqqnrXxCYUo/iqrsM/C4eUSDvydSvnP55Zj+2zHRMRwep4+uY/KSwyjlbxc1mAuzYN7/OzxvuRNBPcag8vxpFKz/ChqdHu4dbrvq/pKlCoWb5sK1XR9oja71eg1JsqFg/dcwhrWDISAKAODS5ma4JfRD1nfPQeNkgP+w56DVG5G/+jP4DXsOJft/R8m+FdC5eMJ3yJMwBEQ25bdNCvDznjTcFh+IwQnBoqM4LJbCNfy6L537GtVGkmAMjoFP3/EAAENQNMy5qSg58PtVpSBZLbiw7F0AgN/gJ+r9EvlrPkfVhVQE3/tejdu9e98L79732j8v3LoQzlGdoNHqULT9Z4Q+9CkqknYhb+VHCHlwdkO/Q1KwV349jE4R3gj0cBYdxSFx+qgOF0oqMX0Fh5q10bn7QO8fUeM2vV84rMU1C/RSIViKchA49s16jxLy136OijO7EXTP23Dy9K/zfua8NJQd2wjvW++D6dxhOLdqD52rF1xvuhVV2WcuTlkR1ZRfVoXXlx4RHcNhsRTqMHX5ERSWc1Ot2hjD2sGcn17jNnN+Ro1FXnshFGQiaNwM6Fw8r/u8kiQhf+3nKD+1HUHjZkDvXfcQX5Ik5K3+FD4DJkJrcAEkGyTbxWm+S39KPDadarf6aDY2nODZzrVhKdRi1ZHz+P0wz1qui+cto1CZeRJF2xfBXJCJsmObUHpwFdy7DANwsRCWvoOqrCT4j3gRsNlgLS2AtbQAkvWvos3+aTKK9/5m/zx/7ecoPboJ/iMmQWtwtT/GZr764kWlB1dD5+IJ15juAABjWDxMqYdQmXECxbuXQe8XYV+gJqrN1OVHeX3nWvDktSsUlZsxcOZmXkXtOsqTdqFw83cwF2TCySsInrfcCY9OtwMALEXZyPji4VofF3TP23CO6AgASP/8Ibh3uM2+RpD6/4bX+hi/O56Fe4eB9s+tZQU4P/8FBN/3fo2T4Qq3/YiSPcuhdfWC/7DnYAzlyUp0bU/0j8akITeJjuFQWApXePXXw/hx1znRMYioBRh0Wvz+zK2ICeSo8hJOH13mWGYxft7NQiBSiyqrjYvOV2ApXObNFcdg47iJSFW2J+dh6f4M0TEcBkvhojVHs7A9OU90DCIS4K2Vx1FUwaMNAZYCAKDKYsPb3BKbSLVySyvx2cYk0TEcAksB1RfOOZvHE52I1Gzen2eRWcgrKqq+FPLLqvDxBl7LlUjtKi02fLjmlOgYwqm+FD5aexIlJm54R0TAkv3pOH6+WHQMoVRdCql5ZfhpV5roGETkIGwS8OGak6JjCKXqUvh0YxIsPAaViC6z7ngO9p8rEB1DGNWWQnpBOZbw2GQiqsUHKh4tqLYUPtt0BmYrRwlEdLVtSXnYfkad5y2pshTOF1Vg8Z7069+RiFTrU5Wet6DKUvhi0xlUWbnXPhHVbWtSLo5mFomO0eJUVwo5JSb8tJtHHBHR9X29JVl0hBanulL4anMyKi0cJRDR9a04dF51ZzmrqhSKTWYs5LUSiKieLDYJ/9maIjpGi1JVKSzek47yKl5+j4jq76fdaSg2qWcHVdWUgiRJWLAjVXQMIpKZ0koLFu5UzwyDakpha1IuknPLRMcgIhmauy0FVSpZi1RNKczfzlECETVMdnElVh3NEh2jRaiiFDIKK7DhRI7oGEQkY7/sUceh7KoohYU7U2HlxndE1AjbknKRoYLDUxVfClUWG37myWpE1Eg2SR2jBcWXwqqjWcgtrRIdg4gUYPHedEiSsmcdFF8Ky7g9NhE1kfSCCvyp8N1TFV0KheVV2HL6gugYRKQgixQ+haToUvjjSBavmUBETWrVkSwUVSj3DGdFl8JvBzNFRyAiham02LDikHLfWxRbCjnFJuxIVvbcHxGJseqIck9kU2wprDh0Hjw1gYiaw47kPMVOISm2FJZz6oiImonZKmGjQndJUGQppOWX40BaoegYRKRga44pcwpJkaWwWiUbVxGROJtPXoDJrLzrsyiyFDaf4rkJRNS8yqqs2JaUKzpGk1NcKZjMVuxKyRcdg4hUQImzEoorhR3JeahUycUwiEis9cdzFLcDs+JKgVNHRNRS8sqqcDC9UHSMJsVSICJqhJ3JypquVlQppOWXI/kCr8NMRC1HaTsnKKoUuCMqEbW0vakFsFiVs46prFLg1BERtbDSSgsOZxSJjtFkFFUKu88WiI5ARCq0U0GHwSumFFJyy5BfxstuElHLU9K6gmJKYW8qRwlEJMaeswWKOV+BpUBE1EillRYcUci6gmJKgbuiEpFISjmJTRGlYDJbcTq7RHQMIlKx4+eLRUdoEooohaOZxbAoZD6PiOTpWCZLwWEcVsiwjYjk62R2iSIWm5VRChnKaGgiki+T2YaU3FLRMRpNEaWQdEH+/yOISP6OnZf/2qYiSiGZpUBEDkAJ6wqyL4ULJZUoMVlExyAiUsQRSLIvBY4SiMhRsBQcQEour59ARI4hp6QSFVVW0TEaRfalkMxSICIHklFYLjpCo8i/FDh9REQOJL2gQnSERlFAKXCkQESOg6UgkM0m4Vy+vIdqRKQsGYUsBWHyyqq45xEROZQMjhTEyS2tFB2BiKiG9AJ5z16wFIiImhCnjwTKK+U1mYnIseSUVMJstYmO0WCyLgWOFIjI0UgSUFhuFh2jwWRdChdYCkTkgIpNLAUhcks4fUREjkfOm3TKuxQ4UiAiB1RcwZGCEPllHCkQkePh9JEgFWZ570ZIRMpUXMHpIyGqLPI97IuIlKuEIwUxWApE5Ig4fSRIpYXTR0TkeHj0kSAcKRCRIzJb5btRp7xLQcankhORctlkvHuzbEvBZpNk3cZEpFxWSb7vTbItBY4SiMhR2WRcCk6iAzSUnHchJMezMHYTOpf/KToGKUSV+yAAnUTHaBDZloJeJ9tBDjmY32JXokPaD6JjkIK4hCeKjtBgsi0FoxNLgRpHo5GwOmYp2qb9IjoKKY1Gvu9Psk2u0WhYDNRgeq2EjdGLWAjUPLSy/X1bvqUAcLRADeOis2Jz6+8Rlb5MdBRSKq1OdIIGk/W7qlEv3x88ieHmZMWWyLkIzVglOgopmYxHCvJNDsBZL+tOoxbmo7dgQ9iX8MncJjoKKZ3RU3SCBpN1KRidOFKg+gk0mrEu+FN4Zu0SHYXUwNVXdIIGk3kpcKRA19fKuRKrA2bDLfuA6CikFi4sBSFcDRwp0LW1cTVhpc+HcLlwVHQUUhOOFMTwczOKjkAOLN69HEs93oMx75ToKKQ2HCmIEeDBUqDadfEqxc/O70BfkCI6CqmRq4/oBA3GUiDF6eFThO+dZsCpKF10FFIrVz/RCRpM1iu1LAW60gC/AizQvgGnEhYCCaJ1Apy9RKdoMHmXgjtLgf5yR0AuvrFNha4sS3QUUjMX+U4dATIvBX+OFOiivwdl4xPzFGgrckVHIbWT8SIzIPNS4PQRAcD9oRn4wDQFWlOh6ChEsl5PAOS+0MzpI9V7tNU5vFI0HRpzuegoRNV8IkUnaBRZjxQMTlr4uhlExyBBnotIxisFU1kI5Fj8YkQnaBRZlwIAtPZ3Ex2BBHgt6iSezn0DGmul6ChENbEUxIoOYCmozYw2RzAx+y1obGbRUYiu5h8rOkGjyHpNAQCiA9xFR6AW9FH0fvxfxgfQQBIdhagWGsA3WnSIRmEpkGx8EbMTt6fPFh2DqG5e4YDeWXSKRpF9KbTh9JEqzI/9H/qkfS46BtG1+cl7lAAooBQifF1h0GlRZbWJjkLNZHHsWnRNmys6BtH1yXyRGVDAQrOTTosIP1fRMaiZ/Ba7koVA8iHzRWZAAaUA8AgkJdJoJKyJXYIOaT+IjkJUfwqYPlJEKcQFeYiOQE1Ir5WwKfpntE37RXQUohvjHyc6QaPJfk0BABLDvUVHoCbiorNifdQPCE1fJToK0Y1xDwa8w0WnaDRFlELnCHlvVUvV3Jys2BQxFwEZG0RHIbpx4beITtAkFDF95OtmQCQXm2XNR2/B1lZfICCThUAyFd5ddIImoYhSAIBOnEKSrUCjGZtD5sAna5voKEQNx1JwLJ1ZCrLUyrkSGwNnwjNnt+goRA2nMwIhnUSnaBKKWFMAuK4gR9GuFVjh8xFcLhwVHYWocUI7AU7K2MZfMSOFdqGeMDop5ttRvHj3cvzh9S5c8lgIpADh3UQnaDKKeRfV67RoH+YlOgbVQxevUix3mwFDwWnRUYiahkLWEwAFlQIA3BIl7wtmq0EPnyIsMrwBfVGK6ChETYel4JhujfUXHYGuYYBfARZo34BTSYboKERNxycKcA8UnaLJKKoUukb5wEWvEx2DanFHQC6+sU2FrixLdBSiphV1q+gETUpRpWB00qFba04hOZrRwdn4xDwF2opc0VGIml7cHaITNClFlQIA9GkbIDoCXeaB0Ey8X/46tKZC0VGImp6TCxDdX3SKJqW4Uugfx1JwFI+Fp+KNkinQVJWKjkLUPNr0A/QuolM0KcWVQpsAd7T25/UVRHsuIhkv5U+DxlwuOgpR87lJWVNHgAJLAQD6xynnSAA5+nfUSTyd+wY01krRUYiaj0YLtB0qOkWTU2QpDIxnKYjydpvDeDj7LWhsZtFRiJpX2M2Au/KmqxVZCt1a+8LXTRn7kMjJrJh9uCfzXWgkq+goRM1PYUcdXaLIUnDSaXFHh2DRMVTly5iduDP9A2ggiY5C1DJYCvIyqlOY6Aiq8X3sFgxJny06BlHL8W0DBN4kOkWzUGwpdI30QZi3sg4Vc0SLY9fi1rQvRMcgalkKHSUACi4FjUaDEYmhomMo2orYleiaNld0DKKW1+kfohM0G8WWAgCM6sRSaA4ajYS1sUvQPu0H0VGIWl5oFyAoQXSKZqPoUogP8URckIfoGIqi10rYFP0zYtN+ER2FSIwuD4hO0KwUXQoAMJKjhSbjorNiS+v5iExfLjoKkRh6N6DDaNEpmpXiS2FUp1BoNaJTyJ+bkxVbIv+DkIzVoqMQiZNwJ2BU9uyD4kuhlY8rt71oJB+9BVtbfYGAzI2ioxCJpfCpI0AFpQAAD/SMEh1BtgKNZmwOmQOfrG2ioxCJ5d8WiPib6BTNThWl0CfWnzunNkAr50psDJwJz5zdoqMQidf5ftEJWoQqSkGj0eC+v0WKjiEr0a4VWOf3PtwuHBAdhUg8rR5IvEd0ihahilIAgLu7toKrgddvro9493L84fUunPOOiY5C5Bjiblfkjqi1UU0peDrrcWdn7od0PV28SrHcbQYMBadFRyFyHD2eFJ2gxaimFABgfI8o0REcWi+fIiwyvAF9UYroKESOI6KnKhaYL1FVKcQFe+BvbXxFx3BIA/wKMF87DU4lGaKjEDmWW18QnaBFqaoUAODxfjGiIzic4QG5+MY2FbqybNFRiBxLSCIQO1B0ihalulLo0zYAnSO8RcdwGKODs/GxeQq0FbmioxA5nt7Pi07Q4lRXCgDwzG2xoiM4hPGhGXi//HVoTYWioxA5Hv+2QPxI0SlanCpLoV9cIBLDvUXHEOqx8FRMK5kKTVWp6ChEjqnXs4BWfW+R6vuOL3rmNvWuLbwQeQYv5U+DxlwuOgqRY/KKADqOFZ1CCNWWwoCbgtCxlZfoGC3u9dYn8OSF6dBYK0VHIXJcvZ4GdE6iUwih2lIAgKcHqGtt4e02h/FQ1gxobGbRUYgcl1ugavY5qo2qS2FguyC0D/MUHaNFzIrZh3sy34VGsoqOQuTY+k8G9M6iUwij6lIAgMlD40VHaHZfxezAnekfQANJdBQixxbUAegyXnQKoVRfCj1j/DG4XZDoGM3m+9gtGJz+segYRPJw+zuqPOLocur+7i96bVg8DDrl/SgWt12HW9O+EB2DSB5uGg60vlV0CuGU907YAJF+bpjQK0p0jCa1MnYFup77j+gYRPKgMwCD3xSdwiGwFC56ckAM/N0NomM0mk5jw9rYX5GQtlB0FCL56P4vwLeN6BQOgaVwkYezHi8OjhMdo1H0WgkbohchNm2x6ChE8uEWAPSZJDqFw2ApXGZM13C0C5HnIaouOiu2tJ6PyPTloqMQycuAfwPO8vx33xw0kiTxOMXL7EzOw9ivdoiOcUPcnKzYFPEfBGRuFB1FVT7fXYXP91ThbKENAJAQqMOUPgYMjdXjbKENrWfXvq/UotEuuDtBX+vXSqskvLLOhKUnLMirkNDaW4unuxvwr65/TW0+v9qEeQeq4GbQ4N3bnHFvx7+e65ejZsw/ZMZv97g24XeqYMEdgH9uUf0RR5dT53nc19C9jR/u6RaOH3eliY5SLz56CzaEfQmfzG2io6hOK08N3h1oRKyvFhKA7w6YMeqnCux/VIub/LU4/4J7jft/tdeM9/+sxNDYuv/ZPb/ahA0pFiy4ywVR3lqsOWPB4ytNCPXQYGScHr+dNGPhYTPW3O+G03k2PLS8AkNidPB31aLIJOG1DZVY9wALoX40wND3WAhX4E+jFpPviEeol+Of0RhoNGNzyBz4ZLEQRBgRp8cdsXrE+unQ1k+HGbc5w90A7Ei3QqfVINhdW+NjyQkzxrTTw92gqfM5/0yzYnyiAf2inBDlrcU/bzYgMViLXRnVZ6Ifz7WhX5QOXUN1uKeDHp5GDVIKqgf7L6014bGuekR48Z91vdwyEYjsKTqFw+Hfnlp4OOsx464OomNcUyvnSmwK+AieObtFRyEAVpuEn46YUWYGeoTrrvr63kwrDmTZ8HCX2qeNLukZrsPyU2ZkFNsgSRI2plhwKs+GwdHVo4vEIB32ZFpRUCFhb6YVFWYJMb5abD1nwb4sK57uLv8j6FqEdwQwcJroFA6J00d16B8XiL93aYX/7ksXHeUq0a4VWOnzIZxzj4mOonqHs63o8W0ZTBbA3QAsGeuCdgFXl8K3+6sQ769Fz/Br/5ObM9QZ/1xhQquZpXDSAloN8PUIZ/SJrH7ckBgn3NdRj1u+LoWLXoPv7nSBmwF4bKUJ80a54PM9ZszZVQV/Vw2+Gu6MhMCrsxCAER8DRvfr30+FuNB8DUXlZgyauRk5JY6zzXSCRxmWuL8HQ8Fp0VEIQJVVwrkiCUUmCYuPmfHNfjM2P+haoxgqzBJCPizB632MeKGn8ZrP98Gflfh6nxkfDDIi0luLLalWvLrehCVjXTGwTe2F8samShSaJEzorMfg78tx+DE3rDhlwSe7q7D3n3zju0qXB4CRc0SncFicProGL1c93rqzvegYdl28SrDUdQYLwYEYdBrE+Gpxc6gO7wx0RmKQFrN3VNW4z+JjZpSbgQcSrz11VGGWMHl9JT4abMSIOD06BunwZDcDxibo8cGftf9iciLXigWHzXhzgBGbzlrQJ1KHADctxiTose+8DSWV/J2vBu8IYMjbolM4NJbCdQxOCMaIxFDRMdDLpwiLDNOhLzorOgpdg00CKq/Ynfzb/WaMjHNCgNu1/7mZbdUf2ivWoXWa6ue9kiRJeHSFCR8NNsLdoIH14uMvPRcAWNkJf9FogTu/AIweopM4NJZCPUwfmYAQgUcj3eaXj/naaXAqyRCWga726joTtqRacLbQhsPZVry6zoRNZ624t8NfI4KkfBu2pFoxsUvtC8A3fVKKJcerL3rkadSgb6QOk9ZWYtNZC1IKbJh3oArzD5nxfzddPcr4Zp8ZAa4ajIir/lqvCCdsSLFgR7oFM7dXol2AFt7OdR/ppDo9ngSieolO4fC40FwPPm4GzLmnM8Z9tQOW2n5la0bDA3LxseUNaCvyWvR16fpyyiQ8sKQC50sleBk16Bikxer7XDEo+q9/Vv/ZX4VWnhoMjq59wfdkng1Fl03x/DTaBa+ur8S9v1Ygv0JCpJcWMwYY8a+uNUshu9SGGf+rxJ8Pu9lv6xamwws9jBi2sAKBbtWL0HRRUHtgwOuiU8gCF5pvwGebkvDeqpMt9np3B2fhvYo3oKksarHXJFIcJxfgkfVAUILoJLLA6aMb8FjfaPSLC2iR1xofmoH3yqewEIgaa8RsFsINYCncAI1Gg4/GdGr29YXHws9iWslUaKpq3zuHiOqp26NA4ljRKWSFpXCDfN0M+PieznC68hCRJvJCxBm8lP8GNObyZnl+ItWI6AkMmSE6heywFBrglihfPDeobZM/7+utT+DJ3OnQWB3nZDkiWfIIAe6eB+iufW4IXY2l0ECP94vGwPjAJnu+t9scxkNZM6CxmZvsOYlUSWcAxswHPIJEJ5EllkIDaTQazB7XGfFNcFGe2dH7cE/mu9BI1uvfmYiu7fZ3gPBuolPIFkuhEdyMTvjPg10R6HHt/Wyu5auYHRiV8QE04JHBRI3W6b7qLbGpwVgKjRTi5YJvxneFi/7Gd6NcELsZg9M/boZURCoU0gkY9qHoFLLHUmgCHVt546MxidDcwAFJ/41di95pXzZfKCI18QoHxi0E9I5/cSxHx1JoIkM7hGDSkLjr3k+jkbAydgVuTpvbAqmIVMDVD7h/CeAVJjqJIrAUmtDj/WIw+uZWdX5dp7FhbcyvSEhb2IKpiBTM4A7cuxjwjxWdRDFYCk3snbs64NZY/6tu12slbIz+GTFp/xWQikiBdEZg3A9AWBfRSRSFpdDE9Dotvrq/K7pG+thvc9FZsaX1fESk/yYwGZGCaLTA378G2vQTnURxuEtqMyk2mTHuyx1IvVCAzRHfwj9zk+hIRMoxfBbQdYLoFIrEUmhGeaWVsCx/BkGnfhQdhUg5BrwO9HlRdArFYik0t5IsYN4wIC9JdBIi+fvb49VnLFOz4ZpCc/MIBsavAHzbiE5CJG9dHwKGvC06heKxFFqCZ0h1MfhEiU5CJE+9nwOGz8QNnSFKDcLpo5ZUmAZ8NwIoSBGdhEg+bpsK3Pq86BSqwVJoaaUXgIV3A5n7RSchcnAaYNgH3OCuhbEURKgsBRY9AJxZLzoJkWPSOgF3fg50HCM6ieqwFESxmoFlTwKHfhKdhMixODlXXzUtbqjoJKrEUhBJkoB1U4Fts0UnIXIMBnfgnh+B1n1EJ1EtloIj2PEFsPpVQLKJTkIkjqsf8I9fgFY3i06iaiwFR3HkV2DJvwBrpegkRC0vqEP15nY+kaKTqB5LwZGk/A/46V6gskh0EqKWk3AXMOpTwOAqOgmBpeB48s5UH5mUfUR0EqLmpdECt02pPjGNHAZLwRGZK4AVzwMHeTEeUihnL+Dv3wKxg0QnoSuwFBzZnrnAHy9znYGUxT+u+ggjv2jRSagWLAVHl7m/ejqp8JzoJESNF3cHcNdXgNFDdBKqA0tBDioKgF//CZxeIzoJUcNodEDfl4C+L3NTOwfHUpALSQL+9wGw8W2ez0Dy4tsG+L8vgfBuopNQPbAU5ObMRmDpY0DJedFJiK7v5gnAkBmAwU10EqonloIcVRQCq18DDiwQnYSodu5BwMhPgLaDRSehG8RSkLOkdcDyZ4DidNFJiP4SPxIYMRtw9RWdhBqApSB3lSXAmteBvfMA8H8lCWT0Au54D0gcJzoJNQJLQSmSNwPLnwIKU0UnITVq3QcY9RngHS46CTUSS0FJqsqAddOAXV+DowZqER6hwOA3gQ6jRSehJsJSUKLUP4EVzwEXTohOQkqlMwI9ngD6vMgjixSGpaBUNiuw77vq8xrKLohOQ0rS9nbg9neqzz8gxWEpKF1lCbB1FrD9U8BSIToNyZlfDHD7u9zETuFYCmpRlA6sfxM49DO43kA3xOAO9JkE/O1xwMkgOg01M5aC2mTuB1b/G0jdKjoJOTqdAeh0L9DvFcAjWHQaaiEsBbU68TuwdgqQd1p0EnI0OiPQ5YHqi994hYlOQy2MpaBmNitwbBmwbRZw/qDoNCSakwvQdQLQ82nAM0R0GhKEpUDVzmysLofkTaKTUEvTuwG3PFRdBu6BotOQYCwFqinzQHU5HFsOSFbRaag5GTyAbhOBHk8Bbn6i05CDYClQ7fKTgT8/AQ78AFhMotNQU/KKqB4ZdBnPTevoKiwFurbSC8Cur6rLoThDdBpqMA0QPQDo9ggQOwTQakUHIgfFUqD6sdmA5I3V5XBiJUcPcuEWWL1r6c0PAn7RotOQDLAU6MZVFAJH/gscWAhk7BGdhq6k0QExA6sPK217O6BzEp2IZISlQI2Tc6J69HBoEVCaJTqNimmAVrcA8cOBDmN4SCk1GEuBmobNCiStB44vB06vZUG0BK0eiOpdXQQ3DedZx9QkWArU9CQJOH8AOLUGOLWqemsN7rfUNPSu1QvG8SOqp4ZcvEUnIoVhKVDzK82pHj2cWlV9clxlsehE8uIZVn1ls5uGAdG3AQZX0YlIwVgK1LKs5uqLACWtA87tqN5ew1opOpVj8Y8DInsAET2r//SOEJ2IVISlQGJZKqvPok7bAaTtAjL2ASWZolO1HK0TENwRiOwJRPSo/uDZxSQQS4EcT0l29ZpE5v7qj+yj1SfOSTbRyRrHIwTwbwsExF388yYgrAsvZ0kOhaVA8mCpAgpTgfyU6i04ClL++u/CVMBaJTphNY2uerrH/sYfVz0dFNAWcPYSna5WW7Zswfvvv4+9e/fi/PnzWLJkCe68807RsUgQntVC8uBkAPxjqz+uZLNVjyTyk4GS84Cp6LKPwis+v/RRfNmGfxpAq6t+Q7f/qa35udEdcPUH3PwBt4DLPq64zcVXdltIlJWVITExEQ899BDuuusu0XFIMI4USL2slotv+hrRSRyGRqPhSEHlOFIg9eL2D0RXkdc4l4iImhVLgYiI7FgKRERkx1IgIiI7rrQRqVxpaSmSkpLsn6ekpODAgQPw9fVFRAS32FAbHpJKpHKbNm1C//79r7p9/PjxmDdvXssHIqFYCkREZMc1BSIismMpEBGRHUuBiIjsWApERGTHUiAiIjuWAhER2bEUiIjIjqVARER2LAUiIrJjKRARkR1LgYiI7FgKRERkx1IgIiI7lgIREdmxFIiIyI6lQEREdiwFIiKyYykQEZEdS4GIiOxYCkREZMdSICIiO5YCERHZsRSIiMiOpUBERHYsBSIismMpEBGRHUuBiIjsWApERGTHUiAiIjuWAhER2bEUiIjIjqVARER2LAUiIrJjKRARkR1LgYiI7FgKRERkx1IgIiI7lgIREdn9f3Y8d/DTTcO+AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"CompleteResponses.csv\")\n",
        "\n",
        "# Distribution of each column\n",
        "input_features = data.columns[:-1]\n",
        "data[input_features].hist(bins=30, figsize=(15, 10))\n",
        "plt.suptitle(\"Histograms of Input Features\")\n",
        "plt.show()\n",
        "\n",
        "class_counts = data['brand'].value_counts()\n",
        "plt.pie(class_counts, labels=['0', '1'], autopct='%1.1f%%')\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Normalization of data\n",
        "normalized_data = (data - data.min(axis=0)) / (data.max(axis=0) - data.min(axis=0))\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into features (X) and target (y)\n",
        "X = normalized_data.drop(\"brand\", axis=1)\n",
        "y = normalized_data[\"brand\"]\n",
        "\n",
        "# Train the data using Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_shape=(X.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "history = model.fit(X, y, epochs=200, validation_split=0.2, callbacks=[checkpoint])\n",
        "\n",
        "# Print the accuracy\n",
        "best_model = tf.keras.models.load_model(\"best_model.h5\")\n",
        "_, train_accuracy = best_model.evaluate(X, y, verbose=1)\n",
        "print(f\"Train accuracy: {train_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLSXGjfAQ3t9",
        "outputId": "3e66ca35-3b07-4944-e59f-f7825d623842"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.6469 - accuracy: 0.6169\n",
            "Epoch 1: val_accuracy improved from -inf to 0.64747, saving model to best_model.h5\n",
            "248/248 [==============================] - 2s 3ms/step - loss: 0.6452 - accuracy: 0.6175 - val_loss: 0.6065 - val_accuracy: 0.6475\n",
            "Epoch 2/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.5733 - accuracy: 0.6917\n",
            "Epoch 2: val_accuracy improved from 0.64747 to 0.74949, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5712 - accuracy: 0.6939 - val_loss: 0.5166 - val_accuracy: 0.7495\n",
            "Epoch 3/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.4853 - accuracy: 0.7730\n",
            "Epoch 3: val_accuracy improved from 0.74949 to 0.79444, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.4855 - accuracy: 0.7723 - val_loss: 0.4443 - val_accuracy: 0.7944\n",
            "Epoch 4/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.4153 - accuracy: 0.8059\n",
            "Epoch 4: val_accuracy improved from 0.79444 to 0.82828, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.4143 - accuracy: 0.8065 - val_loss: 0.3752 - val_accuracy: 0.8283\n",
            "Epoch 5/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8512\n",
            "Epoch 5: val_accuracy improved from 0.82828 to 0.86414, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3505 - accuracy: 0.8511 - val_loss: 0.3217 - val_accuracy: 0.8641\n",
            "Epoch 6/200\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8652\n",
            "Epoch 6: val_accuracy improved from 0.86414 to 0.87273, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.3124 - accuracy: 0.8652 - val_loss: 0.2947 - val_accuracy: 0.8727\n",
            "Epoch 7/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.2894 - accuracy: 0.8785\n",
            "Epoch 7: val_accuracy improved from 0.87273 to 0.87727, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2885 - accuracy: 0.8785 - val_loss: 0.2800 - val_accuracy: 0.8773\n",
            "Epoch 8/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.2775 - accuracy: 0.8798\n",
            "Epoch 8: val_accuracy improved from 0.87727 to 0.89293, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2778 - accuracy: 0.8791 - val_loss: 0.2698 - val_accuracy: 0.8929\n",
            "Epoch 9/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.2680 - accuracy: 0.8865\n",
            "Epoch 9: val_accuracy improved from 0.89293 to 0.89798, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2687 - accuracy: 0.8858 - val_loss: 0.2671 - val_accuracy: 0.8980\n",
            "Epoch 10/200\n",
            "230/248 [==========================>...] - ETA: 0s - loss: 0.2650 - accuracy: 0.8852\n",
            "Epoch 10: val_accuracy did not improve from 0.89798\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2645 - accuracy: 0.8865 - val_loss: 0.2594 - val_accuracy: 0.8899\n",
            "Epoch 11/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.2576 - accuracy: 0.8906\n",
            "Epoch 11: val_accuracy did not improve from 0.89798\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2575 - accuracy: 0.8906 - val_loss: 0.2609 - val_accuracy: 0.8924\n",
            "Epoch 12/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.2547 - accuracy: 0.8899\n",
            "Epoch 12: val_accuracy did not improve from 0.89798\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.2552 - accuracy: 0.8897 - val_loss: 0.2512 - val_accuracy: 0.8889\n",
            "Epoch 13/200\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.2535 - accuracy: 0.8893\n",
            "Epoch 13: val_accuracy did not improve from 0.89798\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2510 - accuracy: 0.8906 - val_loss: 0.2489 - val_accuracy: 0.8970\n",
            "Epoch 14/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.2498 - accuracy: 0.8921\n",
            "Epoch 14: val_accuracy did not improve from 0.89798\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2486 - accuracy: 0.8924 - val_loss: 0.2444 - val_accuracy: 0.8919\n",
            "Epoch 15/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.2447 - accuracy: 0.8963\n",
            "Epoch 15: val_accuracy did not improve from 0.89798\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2452 - accuracy: 0.8961 - val_loss: 0.2477 - val_accuracy: 0.8859\n",
            "Epoch 16/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.2420 - accuracy: 0.8956\n",
            "Epoch 16: val_accuracy did not improve from 0.89798\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2420 - accuracy: 0.8956 - val_loss: 0.2472 - val_accuracy: 0.8798\n",
            "Epoch 17/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.8971\n",
            "Epoch 17: val_accuracy did not improve from 0.89798\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2411 - accuracy: 0.8962 - val_loss: 0.2499 - val_accuracy: 0.8970\n",
            "Epoch 18/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.2395 - accuracy: 0.8949\n",
            "Epoch 18: val_accuracy did not improve from 0.89798\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2395 - accuracy: 0.8948 - val_loss: 0.2460 - val_accuracy: 0.8960\n",
            "Epoch 19/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.2348 - accuracy: 0.8997\n",
            "Epoch 19: val_accuracy improved from 0.89798 to 0.90101, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2367 - accuracy: 0.8982 - val_loss: 0.2378 - val_accuracy: 0.9010\n",
            "Epoch 20/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.2370 - accuracy: 0.8979\n",
            "Epoch 20: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2362 - accuracy: 0.8988 - val_loss: 0.2377 - val_accuracy: 0.8985\n",
            "Epoch 21/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.8963\n",
            "Epoch 21: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2345 - accuracy: 0.8963 - val_loss: 0.2330 - val_accuracy: 0.8995\n",
            "Epoch 22/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.2322 - accuracy: 0.8983\n",
            "Epoch 22: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2321 - accuracy: 0.8987 - val_loss: 0.2352 - val_accuracy: 0.8975\n",
            "Epoch 23/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9004\n",
            "Epoch 23: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2301 - accuracy: 0.9014 - val_loss: 0.2384 - val_accuracy: 0.8894\n",
            "Epoch 24/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9028\n",
            "Epoch 24: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2282 - accuracy: 0.9028 - val_loss: 0.2410 - val_accuracy: 0.8965\n",
            "Epoch 25/200\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.2343 - accuracy: 0.8987\n",
            "Epoch 25: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2311 - accuracy: 0.9000 - val_loss: 0.2417 - val_accuracy: 0.8970\n",
            "Epoch 26/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.2255 - accuracy: 0.9001\n",
            "Epoch 26: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2256 - accuracy: 0.9001 - val_loss: 0.2384 - val_accuracy: 0.8944\n",
            "Epoch 27/200\n",
            "223/248 [=========================>....] - ETA: 0s - loss: 0.2270 - accuracy: 0.9005\n",
            "Epoch 27: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2275 - accuracy: 0.9005 - val_loss: 0.2283 - val_accuracy: 0.8970\n",
            "Epoch 28/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.2280 - accuracy: 0.9002\n",
            "Epoch 28: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2277 - accuracy: 0.9002 - val_loss: 0.2362 - val_accuracy: 0.8995\n",
            "Epoch 29/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.2258 - accuracy: 0.9016\n",
            "Epoch 29: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2246 - accuracy: 0.9030 - val_loss: 0.2271 - val_accuracy: 0.8985\n",
            "Epoch 30/200\n",
            "230/248 [==========================>...] - ETA: 0s - loss: 0.2224 - accuracy: 0.9018\n",
            "Epoch 30: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.2220 - accuracy: 0.9015 - val_loss: 0.2331 - val_accuracy: 0.8990\n",
            "Epoch 31/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.2208 - accuracy: 0.9046\n",
            "Epoch 31: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.2211 - accuracy: 0.9043 - val_loss: 0.2244 - val_accuracy: 0.8985\n",
            "Epoch 32/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.2209 - accuracy: 0.9031\n",
            "Epoch 32: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.2217 - accuracy: 0.9033 - val_loss: 0.2266 - val_accuracy: 0.8970\n",
            "Epoch 33/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.2219 - accuracy: 0.9035\n",
            "Epoch 33: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2206 - accuracy: 0.9048 - val_loss: 0.2308 - val_accuracy: 0.8990\n",
            "Epoch 34/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.2202 - accuracy: 0.9022\n",
            "Epoch 34: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2200 - accuracy: 0.9026 - val_loss: 0.2303 - val_accuracy: 0.9005\n",
            "Epoch 35/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.2185 - accuracy: 0.9052\n",
            "Epoch 35: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2199 - accuracy: 0.9049 - val_loss: 0.2570 - val_accuracy: 0.8828\n",
            "Epoch 36/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.2195 - accuracy: 0.9061\n",
            "Epoch 36: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2195 - accuracy: 0.9059 - val_loss: 0.2316 - val_accuracy: 0.9005\n",
            "Epoch 37/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.2165 - accuracy: 0.9062\n",
            "Epoch 37: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2172 - accuracy: 0.9058 - val_loss: 0.2266 - val_accuracy: 0.8985\n",
            "Epoch 38/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.2159 - accuracy: 0.9047\n",
            "Epoch 38: val_accuracy did not improve from 0.90101\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2160 - accuracy: 0.9050 - val_loss: 0.2246 - val_accuracy: 0.8985\n",
            "Epoch 39/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.2188 - accuracy: 0.9064\n",
            "Epoch 39: val_accuracy improved from 0.90101 to 0.90303, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2186 - accuracy: 0.9065 - val_loss: 0.2188 - val_accuracy: 0.9030\n",
            "Epoch 40/200\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.2119 - accuracy: 0.9087\n",
            "Epoch 40: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2126 - accuracy: 0.9079 - val_loss: 0.2214 - val_accuracy: 0.9015\n",
            "Epoch 41/200\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.2139 - accuracy: 0.9077\n",
            "Epoch 41: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2141 - accuracy: 0.9070 - val_loss: 0.2210 - val_accuracy: 0.8970\n",
            "Epoch 42/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.2135 - accuracy: 0.9051\n",
            "Epoch 42: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2124 - accuracy: 0.9055 - val_loss: 0.2212 - val_accuracy: 0.9010\n",
            "Epoch 43/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.2160 - accuracy: 0.9049\n",
            "Epoch 43: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2142 - accuracy: 0.9064 - val_loss: 0.2296 - val_accuracy: 0.9010\n",
            "Epoch 44/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9089\n",
            "Epoch 44: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2130 - accuracy: 0.9087 - val_loss: 0.2192 - val_accuracy: 0.8985\n",
            "Epoch 45/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.2127 - accuracy: 0.9067\n",
            "Epoch 45: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2109 - accuracy: 0.9084 - val_loss: 0.2248 - val_accuracy: 0.8990\n",
            "Epoch 46/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.2089 - accuracy: 0.9079\n",
            "Epoch 46: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2090 - accuracy: 0.9074 - val_loss: 0.2242 - val_accuracy: 0.9020\n",
            "Epoch 47/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.2114 - accuracy: 0.9074\n",
            "Epoch 47: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2113 - accuracy: 0.9077 - val_loss: 0.2207 - val_accuracy: 0.9025\n",
            "Epoch 48/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.2092 - accuracy: 0.9089\n",
            "Epoch 48: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2110 - accuracy: 0.9082 - val_loss: 0.2183 - val_accuracy: 0.9015\n",
            "Epoch 49/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.2088 - accuracy: 0.9107\n",
            "Epoch 49: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.2088 - accuracy: 0.9107 - val_loss: 0.2153 - val_accuracy: 0.9020\n",
            "Epoch 50/200\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.2112 - accuracy: 0.9089\n",
            "Epoch 50: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.2099 - accuracy: 0.9100 - val_loss: 0.2155 - val_accuracy: 0.8960\n",
            "Epoch 51/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.2054 - accuracy: 0.9099\n",
            "Epoch 51: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2079 - accuracy: 0.9091 - val_loss: 0.2167 - val_accuracy: 0.9030\n",
            "Epoch 52/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.2104 - accuracy: 0.9090\n",
            "Epoch 52: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2108 - accuracy: 0.9091 - val_loss: 0.2190 - val_accuracy: 0.9025\n",
            "Epoch 53/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.2086 - accuracy: 0.9066\n",
            "Epoch 53: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2085 - accuracy: 0.9070 - val_loss: 0.2195 - val_accuracy: 0.8985\n",
            "Epoch 54/200\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.2057 - accuracy: 0.9109\n",
            "Epoch 54: val_accuracy did not improve from 0.90303\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2076 - accuracy: 0.9094 - val_loss: 0.2151 - val_accuracy: 0.9010\n",
            "Epoch 55/200\n",
            "234/248 [===========================>..] - ETA: 0s - loss: 0.2060 - accuracy: 0.9124\n",
            "Epoch 55: val_accuracy improved from 0.90303 to 0.90606, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2051 - accuracy: 0.9126 - val_loss: 0.2144 - val_accuracy: 0.9061\n",
            "Epoch 56/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.2087 - accuracy: 0.9051\n",
            "Epoch 56: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2079 - accuracy: 0.9057 - val_loss: 0.2213 - val_accuracy: 0.9025\n",
            "Epoch 57/200\n",
            "230/248 [==========================>...] - ETA: 0s - loss: 0.2076 - accuracy: 0.9098\n",
            "Epoch 57: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2069 - accuracy: 0.9107 - val_loss: 0.2155 - val_accuracy: 0.9051\n",
            "Epoch 58/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.2041 - accuracy: 0.9144\n",
            "Epoch 58: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2048 - accuracy: 0.9139 - val_loss: 0.2106 - val_accuracy: 0.9005\n",
            "Epoch 59/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.2055 - accuracy: 0.9098\n",
            "Epoch 59: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2062 - accuracy: 0.9094 - val_loss: 0.2136 - val_accuracy: 0.9040\n",
            "Epoch 60/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.2062 - accuracy: 0.9102\n",
            "Epoch 60: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2060 - accuracy: 0.9102 - val_loss: 0.2167 - val_accuracy: 0.9035\n",
            "Epoch 61/200\n",
            "223/248 [=========================>....] - ETA: 0s - loss: 0.2052 - accuracy: 0.9089\n",
            "Epoch 61: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2047 - accuracy: 0.9101 - val_loss: 0.2204 - val_accuracy: 0.9010\n",
            "Epoch 62/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9120\n",
            "Epoch 62: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2022 - accuracy: 0.9118 - val_loss: 0.2106 - val_accuracy: 0.9020\n",
            "Epoch 63/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.2039 - accuracy: 0.9118\n",
            "Epoch 63: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2041 - accuracy: 0.9112 - val_loss: 0.2210 - val_accuracy: 0.9061\n",
            "Epoch 64/200\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.2060 - accuracy: 0.9096\n",
            "Epoch 64: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2067 - accuracy: 0.9092 - val_loss: 0.2125 - val_accuracy: 0.9020\n",
            "Epoch 65/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.2036 - accuracy: 0.9091\n",
            "Epoch 65: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2032 - accuracy: 0.9100 - val_loss: 0.2073 - val_accuracy: 0.9030\n",
            "Epoch 66/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.2060 - accuracy: 0.9086\n",
            "Epoch 66: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2040 - accuracy: 0.9092 - val_loss: 0.2193 - val_accuracy: 0.9030\n",
            "Epoch 67/200\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.1992 - accuracy: 0.9119\n",
            "Epoch 67: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2007 - accuracy: 0.9112 - val_loss: 0.2116 - val_accuracy: 0.9040\n",
            "Epoch 68/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.2030 - accuracy: 0.9106\n",
            "Epoch 68: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2033 - accuracy: 0.9110 - val_loss: 0.2215 - val_accuracy: 0.9015\n",
            "Epoch 69/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.2026 - accuracy: 0.9116\n",
            "Epoch 69: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2018 - accuracy: 0.9120 - val_loss: 0.2087 - val_accuracy: 0.9010\n",
            "Epoch 70/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.2005 - accuracy: 0.9093\n",
            "Epoch 70: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2004 - accuracy: 0.9096 - val_loss: 0.2144 - val_accuracy: 0.9015\n",
            "Epoch 71/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1995 - accuracy: 0.9104\n",
            "Epoch 71: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1999 - accuracy: 0.9102 - val_loss: 0.2146 - val_accuracy: 0.9045\n",
            "Epoch 72/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9129\n",
            "Epoch 72: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.2006 - accuracy: 0.9129 - val_loss: 0.2166 - val_accuracy: 0.9061\n",
            "Epoch 73/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9123\n",
            "Epoch 73: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1992 - accuracy: 0.9124 - val_loss: 0.2229 - val_accuracy: 0.9025\n",
            "Epoch 74/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.1963 - accuracy: 0.9145\n",
            "Epoch 74: val_accuracy did not improve from 0.90606\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1986 - accuracy: 0.9130 - val_loss: 0.2074 - val_accuracy: 0.9005\n",
            "Epoch 75/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1996 - accuracy: 0.9116\n",
            "Epoch 75: val_accuracy improved from 0.90606 to 0.90657, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2007 - accuracy: 0.9110 - val_loss: 0.2109 - val_accuracy: 0.9066\n",
            "Epoch 76/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1981 - accuracy: 0.9123\n",
            "Epoch 76: val_accuracy did not improve from 0.90657\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1987 - accuracy: 0.9121 - val_loss: 0.2113 - val_accuracy: 0.9025\n",
            "Epoch 77/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1971 - accuracy: 0.9133\n",
            "Epoch 77: val_accuracy did not improve from 0.90657\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1974 - accuracy: 0.9122 - val_loss: 0.2264 - val_accuracy: 0.9000\n",
            "Epoch 78/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1985 - accuracy: 0.9148\n",
            "Epoch 78: val_accuracy did not improve from 0.90657\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1980 - accuracy: 0.9148 - val_loss: 0.2061 - val_accuracy: 0.9020\n",
            "Epoch 79/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.1998 - accuracy: 0.9129\n",
            "Epoch 79: val_accuracy did not improve from 0.90657\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1964 - accuracy: 0.9144 - val_loss: 0.2094 - val_accuracy: 0.9045\n",
            "Epoch 80/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.1951 - accuracy: 0.9157\n",
            "Epoch 80: val_accuracy improved from 0.90657 to 0.90859, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1962 - accuracy: 0.9154 - val_loss: 0.2132 - val_accuracy: 0.9086\n",
            "Epoch 81/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1933 - accuracy: 0.9155\n",
            "Epoch 81: val_accuracy improved from 0.90859 to 0.90909, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1928 - accuracy: 0.9159 - val_loss: 0.2040 - val_accuracy: 0.9091\n",
            "Epoch 82/200\n",
            "224/248 [==========================>...] - ETA: 0s - loss: 0.1955 - accuracy: 0.9150\n",
            "Epoch 82: val_accuracy did not improve from 0.90909\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1956 - accuracy: 0.9140 - val_loss: 0.2013 - val_accuracy: 0.9035\n",
            "Epoch 83/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1955 - accuracy: 0.9152\n",
            "Epoch 83: val_accuracy did not improve from 0.90909\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1941 - accuracy: 0.9160 - val_loss: 0.2086 - val_accuracy: 0.9035\n",
            "Epoch 84/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1933 - accuracy: 0.9141\n",
            "Epoch 84: val_accuracy did not improve from 0.90909\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1935 - accuracy: 0.9140 - val_loss: 0.2033 - val_accuracy: 0.9076\n",
            "Epoch 85/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1907 - accuracy: 0.9177\n",
            "Epoch 85: val_accuracy did not improve from 0.90909\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1903 - accuracy: 0.9179 - val_loss: 0.2070 - val_accuracy: 0.9056\n",
            "Epoch 86/200\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9145\n",
            "Epoch 86: val_accuracy improved from 0.90909 to 0.90960, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1929 - accuracy: 0.9145 - val_loss: 0.1974 - val_accuracy: 0.9096\n",
            "Epoch 87/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.9161\n",
            "Epoch 87: val_accuracy did not improve from 0.90960\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1902 - accuracy: 0.9161 - val_loss: 0.2003 - val_accuracy: 0.9056\n",
            "Epoch 88/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1922 - accuracy: 0.9112\n",
            "Epoch 88: val_accuracy did not improve from 0.90960\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1919 - accuracy: 0.9125 - val_loss: 0.2018 - val_accuracy: 0.9056\n",
            "Epoch 89/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9150\n",
            "Epoch 89: val_accuracy did not improve from 0.90960\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1896 - accuracy: 0.9150 - val_loss: 0.2022 - val_accuracy: 0.9071\n",
            "Epoch 90/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1863 - accuracy: 0.9184\n",
            "Epoch 90: val_accuracy did not improve from 0.90960\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1869 - accuracy: 0.9185 - val_loss: 0.1943 - val_accuracy: 0.9076\n",
            "Epoch 91/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9171\n",
            "Epoch 91: val_accuracy did not improve from 0.90960\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9170 - val_loss: 0.1971 - val_accuracy: 0.9061\n",
            "Epoch 92/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1907 - accuracy: 0.9151\n",
            "Epoch 92: val_accuracy did not improve from 0.90960\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1887 - accuracy: 0.9160 - val_loss: 0.1960 - val_accuracy: 0.9086\n",
            "Epoch 93/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1849 - accuracy: 0.9180\n",
            "Epoch 93: val_accuracy did not improve from 0.90960\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1850 - accuracy: 0.9187 - val_loss: 0.1981 - val_accuracy: 0.9081\n",
            "Epoch 94/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.1837 - accuracy: 0.9178\n",
            "Epoch 94: val_accuracy did not improve from 0.90960\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1844 - accuracy: 0.9179 - val_loss: 0.1964 - val_accuracy: 0.9071\n",
            "Epoch 95/200\n",
            "224/248 [==========================>...] - ETA: 0s - loss: 0.1844 - accuracy: 0.9195\n",
            "Epoch 95: val_accuracy improved from 0.90960 to 0.91212, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1847 - accuracy: 0.9189 - val_loss: 0.1989 - val_accuracy: 0.9121\n",
            "Epoch 96/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1843 - accuracy: 0.9176\n",
            "Epoch 96: val_accuracy did not improve from 0.91212\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1834 - accuracy: 0.9180 - val_loss: 0.1900 - val_accuracy: 0.9101\n",
            "Epoch 97/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1828 - accuracy: 0.9198\n",
            "Epoch 97: val_accuracy did not improve from 0.91212\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1831 - accuracy: 0.9203 - val_loss: 0.2031 - val_accuracy: 0.9116\n",
            "Epoch 98/200\n",
            "232/248 [===========================>..] - ETA: 0s - loss: 0.1799 - accuracy: 0.9199\n",
            "Epoch 98: val_accuracy did not improve from 0.91212\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1828 - accuracy: 0.9188 - val_loss: 0.1929 - val_accuracy: 0.9086\n",
            "Epoch 99/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1830 - accuracy: 0.9184\n",
            "Epoch 99: val_accuracy did not improve from 0.91212\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1822 - accuracy: 0.9188 - val_loss: 0.2065 - val_accuracy: 0.9106\n",
            "Epoch 100/200\n",
            "232/248 [===========================>..] - ETA: 0s - loss: 0.1791 - accuracy: 0.9204\n",
            "Epoch 100: val_accuracy did not improve from 0.91212\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1819 - accuracy: 0.9193 - val_loss: 0.2038 - val_accuracy: 0.9051\n",
            "Epoch 101/200\n",
            "234/248 [===========================>..] - ETA: 0s - loss: 0.1829 - accuracy: 0.9207\n",
            "Epoch 101: val_accuracy improved from 0.91212 to 0.91364, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1832 - accuracy: 0.9202 - val_loss: 0.1911 - val_accuracy: 0.9136\n",
            "Epoch 102/200\n",
            "234/248 [===========================>..] - ETA: 0s - loss: 0.1796 - accuracy: 0.9209\n",
            "Epoch 102: val_accuracy did not improve from 0.91364\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1801 - accuracy: 0.9212 - val_loss: 0.2065 - val_accuracy: 0.9061\n",
            "Epoch 103/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.1802 - accuracy: 0.9214\n",
            "Epoch 103: val_accuracy did not improve from 0.91364\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1804 - accuracy: 0.9219 - val_loss: 0.1874 - val_accuracy: 0.9096\n",
            "Epoch 104/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1799 - accuracy: 0.9184\n",
            "Epoch 104: val_accuracy improved from 0.91364 to 0.91616, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1794 - accuracy: 0.9192 - val_loss: 0.1883 - val_accuracy: 0.9162\n",
            "Epoch 105/200\n",
            "232/248 [===========================>..] - ETA: 0s - loss: 0.1826 - accuracy: 0.9199\n",
            "Epoch 105: val_accuracy did not improve from 0.91616\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1812 - accuracy: 0.9201 - val_loss: 0.1890 - val_accuracy: 0.9146\n",
            "Epoch 106/200\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.1783 - accuracy: 0.9217\n",
            "Epoch 106: val_accuracy did not improve from 0.91616\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1775 - accuracy: 0.9225 - val_loss: 0.1862 - val_accuracy: 0.9146\n",
            "Epoch 107/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1778 - accuracy: 0.9224\n",
            "Epoch 107: val_accuracy did not improve from 0.91616\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1779 - accuracy: 0.9226 - val_loss: 0.1880 - val_accuracy: 0.9136\n",
            "Epoch 108/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.9202\n",
            "Epoch 108: val_accuracy did not improve from 0.91616\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1760 - accuracy: 0.9207 - val_loss: 0.1889 - val_accuracy: 0.9136\n",
            "Epoch 109/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9228\n",
            "Epoch 109: val_accuracy did not improve from 0.91616\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1755 - accuracy: 0.9226 - val_loss: 0.2051 - val_accuracy: 0.9051\n",
            "Epoch 110/200\n",
            "224/248 [==========================>...] - ETA: 0s - loss: 0.1796 - accuracy: 0.9213\n",
            "Epoch 110: val_accuracy did not improve from 0.91616\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1809 - accuracy: 0.9208 - val_loss: 0.1833 - val_accuracy: 0.9146\n",
            "Epoch 111/200\n",
            "232/248 [===========================>..] - ETA: 0s - loss: 0.1778 - accuracy: 0.9196\n",
            "Epoch 111: val_accuracy did not improve from 0.91616\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.9203 - val_loss: 0.1850 - val_accuracy: 0.9121\n",
            "Epoch 112/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.1738 - accuracy: 0.9234\n",
            "Epoch 112: val_accuracy improved from 0.91616 to 0.91667, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1765 - accuracy: 0.9217 - val_loss: 0.1918 - val_accuracy: 0.9167\n",
            "Epoch 113/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.1765 - accuracy: 0.9214\n",
            "Epoch 113: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1774 - accuracy: 0.9203 - val_loss: 0.2068 - val_accuracy: 0.9051\n",
            "Epoch 114/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.1784 - accuracy: 0.9198\n",
            "Epoch 114: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1767 - accuracy: 0.9207 - val_loss: 0.1865 - val_accuracy: 0.9131\n",
            "Epoch 115/200\n",
            "230/248 [==========================>...] - ETA: 0s - loss: 0.1758 - accuracy: 0.9205\n",
            "Epoch 115: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1761 - accuracy: 0.9209 - val_loss: 0.1823 - val_accuracy: 0.9131\n",
            "Epoch 116/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.1745 - accuracy: 0.9242\n",
            "Epoch 116: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1742 - accuracy: 0.9238 - val_loss: 0.1871 - val_accuracy: 0.9126\n",
            "Epoch 117/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1758 - accuracy: 0.9218\n",
            "Epoch 117: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1764 - accuracy: 0.9209 - val_loss: 0.1818 - val_accuracy: 0.9131\n",
            "Epoch 118/200\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.1719 - accuracy: 0.9245\n",
            "Epoch 118: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1743 - accuracy: 0.9240 - val_loss: 0.1920 - val_accuracy: 0.9106\n",
            "Epoch 119/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.1727 - accuracy: 0.9227\n",
            "Epoch 119: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1718 - accuracy: 0.9238 - val_loss: 0.1847 - val_accuracy: 0.9157\n",
            "Epoch 120/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1786 - accuracy: 0.9215\n",
            "Epoch 120: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1778 - accuracy: 0.9219 - val_loss: 0.1974 - val_accuracy: 0.9101\n",
            "Epoch 121/200\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.1759 - accuracy: 0.9222\n",
            "Epoch 121: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1755 - accuracy: 0.9225 - val_loss: 0.1814 - val_accuracy: 0.9162\n",
            "Epoch 122/200\n",
            "223/248 [=========================>....] - ETA: 0s - loss: 0.1749 - accuracy: 0.9219\n",
            "Epoch 122: val_accuracy did not improve from 0.91667\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1751 - accuracy: 0.9221 - val_loss: 0.1853 - val_accuracy: 0.9121\n",
            "Epoch 123/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1753 - accuracy: 0.9191\n",
            "Epoch 123: val_accuracy improved from 0.91667 to 0.91818, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1760 - accuracy: 0.9189 - val_loss: 0.1816 - val_accuracy: 0.9182\n",
            "Epoch 124/200\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.1705 - accuracy: 0.9225\n",
            "Epoch 124: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1719 - accuracy: 0.9219 - val_loss: 0.1786 - val_accuracy: 0.9141\n",
            "Epoch 125/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9220\n",
            "Epoch 125: val_accuracy improved from 0.91818 to 0.92071, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1725 - accuracy: 0.9221 - val_loss: 0.1786 - val_accuracy: 0.9207\n",
            "Epoch 126/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.9239\n",
            "Epoch 126: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1723 - accuracy: 0.9241 - val_loss: 0.1781 - val_accuracy: 0.9167\n",
            "Epoch 127/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9240\n",
            "Epoch 127: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1706 - accuracy: 0.9240 - val_loss: 0.1796 - val_accuracy: 0.9157\n",
            "Epoch 128/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9231\n",
            "Epoch 128: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1723 - accuracy: 0.9233 - val_loss: 0.1802 - val_accuracy: 0.9167\n",
            "Epoch 129/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9251\n",
            "Epoch 129: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1706 - accuracy: 0.9256 - val_loss: 0.1789 - val_accuracy: 0.9172\n",
            "Epoch 130/200\n",
            "222/248 [=========================>....] - ETA: 0s - loss: 0.1650 - accuracy: 0.9268\n",
            "Epoch 130: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1688 - accuracy: 0.9249 - val_loss: 0.1835 - val_accuracy: 0.9157\n",
            "Epoch 131/200\n",
            "226/248 [==========================>...] - ETA: 0s - loss: 0.1707 - accuracy: 0.9239\n",
            "Epoch 131: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1709 - accuracy: 0.9246 - val_loss: 0.1778 - val_accuracy: 0.9187\n",
            "Epoch 132/200\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.1735 - accuracy: 0.9228\n",
            "Epoch 132: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1721 - accuracy: 0.9226 - val_loss: 0.1866 - val_accuracy: 0.9116\n",
            "Epoch 133/200\n",
            "225/248 [==========================>...] - ETA: 0s - loss: 0.1729 - accuracy: 0.9207\n",
            "Epoch 133: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1713 - accuracy: 0.9222 - val_loss: 0.1844 - val_accuracy: 0.9146\n",
            "Epoch 134/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1684 - accuracy: 0.9261\n",
            "Epoch 134: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1695 - accuracy: 0.9252 - val_loss: 0.1765 - val_accuracy: 0.9197\n",
            "Epoch 135/200\n",
            "225/248 [==========================>...] - ETA: 0s - loss: 0.1683 - accuracy: 0.9251\n",
            "Epoch 135: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1689 - accuracy: 0.9247 - val_loss: 0.1952 - val_accuracy: 0.9111\n",
            "Epoch 136/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1659 - accuracy: 0.9241\n",
            "Epoch 136: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1670 - accuracy: 0.9233 - val_loss: 0.1791 - val_accuracy: 0.9197\n",
            "Epoch 137/200\n",
            "224/248 [==========================>...] - ETA: 0s - loss: 0.1691 - accuracy: 0.9240\n",
            "Epoch 137: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1693 - accuracy: 0.9245 - val_loss: 0.1907 - val_accuracy: 0.9126\n",
            "Epoch 138/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9235\n",
            "Epoch 138: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1676 - accuracy: 0.9235 - val_loss: 0.1973 - val_accuracy: 0.9111\n",
            "Epoch 139/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1705 - accuracy: 0.9226\n",
            "Epoch 139: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1701 - accuracy: 0.9238 - val_loss: 0.1774 - val_accuracy: 0.9162\n",
            "Epoch 140/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.9237\n",
            "Epoch 140: val_accuracy did not improve from 0.92071\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1646 - accuracy: 0.9238 - val_loss: 0.1802 - val_accuracy: 0.9162\n",
            "Epoch 141/200\n",
            "223/248 [=========================>....] - ETA: 0s - loss: 0.1649 - accuracy: 0.9252\n",
            "Epoch 141: val_accuracy improved from 0.92071 to 0.92172, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1660 - accuracy: 0.9250 - val_loss: 0.1777 - val_accuracy: 0.9217\n",
            "Epoch 142/200\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.1651 - accuracy: 0.9261\n",
            "Epoch 142: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1692 - accuracy: 0.9238 - val_loss: 0.2142 - val_accuracy: 0.9066\n",
            "Epoch 143/200\n",
            "222/248 [=========================>....] - ETA: 0s - loss: 0.1719 - accuracy: 0.9248\n",
            "Epoch 143: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1694 - accuracy: 0.9261 - val_loss: 0.1776 - val_accuracy: 0.9182\n",
            "Epoch 144/200\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.1666 - accuracy: 0.9258\n",
            "Epoch 144: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1654 - accuracy: 0.9265 - val_loss: 0.1797 - val_accuracy: 0.9167\n",
            "Epoch 145/200\n",
            "234/248 [===========================>..] - ETA: 0s - loss: 0.1679 - accuracy: 0.9244\n",
            "Epoch 145: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1670 - accuracy: 0.9250 - val_loss: 0.1955 - val_accuracy: 0.9106\n",
            "Epoch 146/200\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.1677 - accuracy: 0.9234\n",
            "Epoch 146: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1669 - accuracy: 0.9236 - val_loss: 0.1736 - val_accuracy: 0.9207\n",
            "Epoch 147/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1672 - accuracy: 0.9243\n",
            "Epoch 147: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1665 - accuracy: 0.9254 - val_loss: 0.1817 - val_accuracy: 0.9207\n",
            "Epoch 148/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1659 - accuracy: 0.9251\n",
            "Epoch 148: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1639 - accuracy: 0.9265 - val_loss: 0.1792 - val_accuracy: 0.9177\n",
            "Epoch 149/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1656 - accuracy: 0.9265\n",
            "Epoch 149: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1658 - accuracy: 0.9262 - val_loss: 0.1753 - val_accuracy: 0.9167\n",
            "Epoch 150/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1641 - accuracy: 0.9255\n",
            "Epoch 150: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1635 - accuracy: 0.9256 - val_loss: 0.1716 - val_accuracy: 0.9157\n",
            "Epoch 151/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1669 - accuracy: 0.9251\n",
            "Epoch 151: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1666 - accuracy: 0.9252 - val_loss: 0.1740 - val_accuracy: 0.9202\n",
            "Epoch 152/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.9261\n",
            "Epoch 152: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1650 - accuracy: 0.9261 - val_loss: 0.2073 - val_accuracy: 0.9071\n",
            "Epoch 153/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9257\n",
            "Epoch 153: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1656 - accuracy: 0.9257 - val_loss: 0.1829 - val_accuracy: 0.9187\n",
            "Epoch 154/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1653 - accuracy: 0.9230\n",
            "Epoch 154: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1657 - accuracy: 0.9227 - val_loss: 0.1826 - val_accuracy: 0.9177\n",
            "Epoch 155/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.9247\n",
            "Epoch 155: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1647 - accuracy: 0.9243 - val_loss: 0.1834 - val_accuracy: 0.9146\n",
            "Epoch 156/200\n",
            "234/248 [===========================>..] - ETA: 0s - loss: 0.1659 - accuracy: 0.9245\n",
            "Epoch 156: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1653 - accuracy: 0.9247 - val_loss: 0.1791 - val_accuracy: 0.9192\n",
            "Epoch 157/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1652 - accuracy: 0.9267\n",
            "Epoch 157: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1657 - accuracy: 0.9261 - val_loss: 0.1923 - val_accuracy: 0.9111\n",
            "Epoch 158/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9255\n",
            "Epoch 158: val_accuracy did not improve from 0.92172\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1635 - accuracy: 0.9256 - val_loss: 0.1861 - val_accuracy: 0.9146\n",
            "Epoch 159/200\n",
            "228/248 [==========================>...] - ETA: 0s - loss: 0.1620 - accuracy: 0.9254\n",
            "Epoch 159: val_accuracy improved from 0.92172 to 0.92374, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1636 - accuracy: 0.9246 - val_loss: 0.1761 - val_accuracy: 0.9237\n",
            "Epoch 160/200\n",
            "223/248 [=========================>....] - ETA: 0s - loss: 0.1634 - accuracy: 0.9242\n",
            "Epoch 160: val_accuracy did not improve from 0.92374\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.1635 - accuracy: 0.9242 - val_loss: 0.1977 - val_accuracy: 0.9091\n",
            "Epoch 161/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1621 - accuracy: 0.9262\n",
            "Epoch 161: val_accuracy did not improve from 0.92374\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1632 - accuracy: 0.9255 - val_loss: 0.1805 - val_accuracy: 0.9146\n",
            "Epoch 162/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.1647 - accuracy: 0.9259\n",
            "Epoch 162: val_accuracy did not improve from 0.92374\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1653 - accuracy: 0.9252 - val_loss: 0.1768 - val_accuracy: 0.9157\n",
            "Epoch 163/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1662 - accuracy: 0.9217\n",
            "Epoch 163: val_accuracy did not improve from 0.92374\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1662 - accuracy: 0.9216 - val_loss: 0.1707 - val_accuracy: 0.9197\n",
            "Epoch 164/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1614 - accuracy: 0.9279\n",
            "Epoch 164: val_accuracy did not improve from 0.92374\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1620 - accuracy: 0.9276 - val_loss: 0.2120 - val_accuracy: 0.9091\n",
            "Epoch 165/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1627 - accuracy: 0.9237\n",
            "Epoch 165: val_accuracy improved from 0.92374 to 0.92475, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1641 - accuracy: 0.9236 - val_loss: 0.1765 - val_accuracy: 0.9247\n",
            "Epoch 166/200\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1642 - accuracy: 0.9251\n",
            "Epoch 166: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1641 - accuracy: 0.9250 - val_loss: 0.1721 - val_accuracy: 0.9217\n",
            "Epoch 167/200\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1650 - accuracy: 0.9239\n",
            "Epoch 167: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1648 - accuracy: 0.9240 - val_loss: 0.1728 - val_accuracy: 0.9212\n",
            "Epoch 168/200\n",
            "230/248 [==========================>...] - ETA: 0s - loss: 0.1632 - accuracy: 0.9268\n",
            "Epoch 168: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1622 - accuracy: 0.9269 - val_loss: 0.1753 - val_accuracy: 0.9187\n",
            "Epoch 169/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1609 - accuracy: 0.9254\n",
            "Epoch 169: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1615 - accuracy: 0.9254 - val_loss: 0.1719 - val_accuracy: 0.9242\n",
            "Epoch 170/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1618 - accuracy: 0.9264\n",
            "Epoch 170: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1619 - accuracy: 0.9265 - val_loss: 0.1724 - val_accuracy: 0.9217\n",
            "Epoch 171/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9257\n",
            "Epoch 171: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1623 - accuracy: 0.9257 - val_loss: 0.1725 - val_accuracy: 0.9207\n",
            "Epoch 172/200\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1666 - accuracy: 0.9238\n",
            "Epoch 172: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1657 - accuracy: 0.9242 - val_loss: 0.1762 - val_accuracy: 0.9197\n",
            "Epoch 173/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1602 - accuracy: 0.9269\n",
            "Epoch 173: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1605 - accuracy: 0.9267 - val_loss: 0.1804 - val_accuracy: 0.9131\n",
            "Epoch 174/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.9245\n",
            "Epoch 174: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1655 - accuracy: 0.9245 - val_loss: 0.1733 - val_accuracy: 0.9222\n",
            "Epoch 175/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1612 - accuracy: 0.9263\n",
            "Epoch 175: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1627 - accuracy: 0.9254 - val_loss: 0.1767 - val_accuracy: 0.9222\n",
            "Epoch 176/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9262\n",
            "Epoch 176: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1598 - accuracy: 0.9262 - val_loss: 0.1693 - val_accuracy: 0.9202\n",
            "Epoch 177/200\n",
            "222/248 [=========================>....] - ETA: 0s - loss: 0.1623 - accuracy: 0.9245\n",
            "Epoch 177: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1628 - accuracy: 0.9249 - val_loss: 0.1763 - val_accuracy: 0.9197\n",
            "Epoch 178/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1596 - accuracy: 0.9276\n",
            "Epoch 178: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1595 - accuracy: 0.9279 - val_loss: 0.1965 - val_accuracy: 0.9131\n",
            "Epoch 179/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.1596 - accuracy: 0.9263\n",
            "Epoch 179: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9266 - val_loss: 0.1781 - val_accuracy: 0.9172\n",
            "Epoch 180/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9252\n",
            "Epoch 180: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1628 - accuracy: 0.9252 - val_loss: 0.1782 - val_accuracy: 0.9197\n",
            "Epoch 181/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1607 - accuracy: 0.9250\n",
            "Epoch 181: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1611 - accuracy: 0.9247 - val_loss: 0.1762 - val_accuracy: 0.9222\n",
            "Epoch 182/200\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1594 - accuracy: 0.9277\n",
            "Epoch 182: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1599 - accuracy: 0.9276 - val_loss: 0.1730 - val_accuracy: 0.9237\n",
            "Epoch 183/200\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.1567 - accuracy: 0.9279\n",
            "Epoch 183: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1584 - accuracy: 0.9278 - val_loss: 0.1764 - val_accuracy: 0.9227\n",
            "Epoch 184/200\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.1634 - accuracy: 0.9252\n",
            "Epoch 184: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1625 - accuracy: 0.9255 - val_loss: 0.1693 - val_accuracy: 0.9187\n",
            "Epoch 185/200\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1579 - accuracy: 0.9256\n",
            "Epoch 185: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1579 - accuracy: 0.9256 - val_loss: 0.1703 - val_accuracy: 0.9247\n",
            "Epoch 186/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1616 - accuracy: 0.9250\n",
            "Epoch 186: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1614 - accuracy: 0.9250 - val_loss: 0.1784 - val_accuracy: 0.9182\n",
            "Epoch 187/200\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9271\n",
            "Epoch 187: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9274 - val_loss: 0.1764 - val_accuracy: 0.9187\n",
            "Epoch 188/200\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1611 - accuracy: 0.9278\n",
            "Epoch 188: val_accuracy did not improve from 0.92475\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1616 - accuracy: 0.9278 - val_loss: 0.1703 - val_accuracy: 0.9227\n",
            "Epoch 189/200\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1619 - accuracy: 0.9254\n",
            "Epoch 189: val_accuracy improved from 0.92475 to 0.92677, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1620 - accuracy: 0.9254 - val_loss: 0.1728 - val_accuracy: 0.9268\n",
            "Epoch 190/200\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.1627 - accuracy: 0.9275\n",
            "Epoch 190: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1617 - accuracy: 0.9281 - val_loss: 0.1778 - val_accuracy: 0.9177\n",
            "Epoch 191/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1577 - accuracy: 0.9272\n",
            "Epoch 191: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1585 - accuracy: 0.9267 - val_loss: 0.1801 - val_accuracy: 0.9172\n",
            "Epoch 192/200\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1554 - accuracy: 0.9297\n",
            "Epoch 192: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1584 - accuracy: 0.9284 - val_loss: 0.1745 - val_accuracy: 0.9202\n",
            "Epoch 193/200\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9277\n",
            "Epoch 193: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1598 - accuracy: 0.9276 - val_loss: 0.1735 - val_accuracy: 0.9167\n",
            "Epoch 194/200\n",
            "223/248 [=========================>....] - ETA: 0s - loss: 0.1576 - accuracy: 0.9268\n",
            "Epoch 194: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1579 - accuracy: 0.9260 - val_loss: 0.1795 - val_accuracy: 0.9172\n",
            "Epoch 195/200\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1598 - accuracy: 0.9242\n",
            "Epoch 195: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1598 - accuracy: 0.9242 - val_loss: 0.1706 - val_accuracy: 0.9227\n",
            "Epoch 196/200\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1579 - accuracy: 0.9271\n",
            "Epoch 196: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1584 - accuracy: 0.9269 - val_loss: 0.1766 - val_accuracy: 0.9232\n",
            "Epoch 197/200\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1601 - accuracy: 0.9267\n",
            "Epoch 197: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1595 - accuracy: 0.9271 - val_loss: 0.1745 - val_accuracy: 0.9227\n",
            "Epoch 198/200\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.1581 - accuracy: 0.9289\n",
            "Epoch 198: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1595 - accuracy: 0.9275 - val_loss: 0.1832 - val_accuracy: 0.9202\n",
            "Epoch 199/200\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9288\n",
            "Epoch 199: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1618 - accuracy: 0.9288 - val_loss: 0.1706 - val_accuracy: 0.9247\n",
            "Epoch 200/200\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1583 - accuracy: 0.9278\n",
            "Epoch 200: val_accuracy did not improve from 0.92677\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1581 - accuracy: 0.9274 - val_loss: 0.1699 - val_accuracy: 0.9207\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 0.1607 - accuracy: 0.9278\n",
            "Train accuracy: 0.9278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the data using Keras\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(256, activation='relu', input_shape=(X.shape[1],)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "history = model.fit(X, y, epochs=200, validation_split=0.2, callbacks=[checkpoint])\n",
        "\n",
        "# Print the accuracy\n",
        "best_model = tf.keras.models.load_model(\"best_model.h5\")\n",
        "_, train_accuracy = best_model.evaluate(X, y, verbose=1)\n",
        "print(f\"Train accuracy: {train_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgE-6OTmTTUD",
        "outputId": "66ebec2e-c850-4d27-98f1-a9791c097a68"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.6902\n",
            "Epoch 1: val_accuracy improved from -inf to 0.77576, saving model to best_model.h5\n",
            "248/248 [==============================] - 3s 6ms/step - loss: 0.5628 - accuracy: 0.6908 - val_loss: 0.4554 - val_accuracy: 0.7758\n",
            "Epoch 2/50\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.3844 - accuracy: 0.8191\n",
            "Epoch 2: val_accuracy improved from 0.77576 to 0.86364, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.3828 - accuracy: 0.8198 - val_loss: 0.3070 - val_accuracy: 0.8636\n",
            "Epoch 3/50\n",
            "236/248 [===========================>..] - ETA: 0s - loss: 0.2867 - accuracy: 0.8731\n",
            "Epoch 3: val_accuracy improved from 0.86364 to 0.87576, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.2866 - accuracy: 0.8733 - val_loss: 0.2650 - val_accuracy: 0.8758\n",
            "Epoch 4/50\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.2633 - accuracy: 0.8844\n",
            "Epoch 4: val_accuracy improved from 0.87576 to 0.89091, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2629 - accuracy: 0.8848 - val_loss: 0.2456 - val_accuracy: 0.8909\n",
            "Epoch 5/50\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.8932\n",
            "Epoch 5: val_accuracy did not improve from 0.89091\n",
            "248/248 [==============================] - 2s 6ms/step - loss: 0.2409 - accuracy: 0.8934 - val_loss: 0.2528 - val_accuracy: 0.8884\n",
            "Epoch 6/50\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.2410 - accuracy: 0.8957\n",
            "Epoch 6: val_accuracy did not improve from 0.89091\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.2410 - accuracy: 0.8957 - val_loss: 0.2921 - val_accuracy: 0.8702\n",
            "Epoch 7/50\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.2397 - accuracy: 0.8928\n",
            "Epoch 7: val_accuracy improved from 0.89091 to 0.89394, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2385 - accuracy: 0.8939 - val_loss: 0.2277 - val_accuracy: 0.8939\n",
            "Epoch 8/50\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.2270 - accuracy: 0.8999\n",
            "Epoch 8: val_accuracy improved from 0.89394 to 0.90303, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2271 - accuracy: 0.9005 - val_loss: 0.2216 - val_accuracy: 0.9030\n",
            "Epoch 9/50\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.2210 - accuracy: 0.9060\n",
            "Epoch 9: val_accuracy improved from 0.90303 to 0.90455, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2197 - accuracy: 0.9069 - val_loss: 0.2140 - val_accuracy: 0.9045\n",
            "Epoch 10/50\n",
            "230/248 [==========================>...] - ETA: 0s - loss: 0.2047 - accuracy: 0.9086\n",
            "Epoch 10: val_accuracy did not improve from 0.90455\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2078 - accuracy: 0.9073 - val_loss: 0.2250 - val_accuracy: 0.9025\n",
            "Epoch 11/50\n",
            "232/248 [===========================>..] - ETA: 0s - loss: 0.2095 - accuracy: 0.9065\n",
            "Epoch 11: val_accuracy did not improve from 0.90455\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.2076 - accuracy: 0.9078 - val_loss: 0.2437 - val_accuracy: 0.8934\n",
            "Epoch 12/50\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9084\n",
            "Epoch 12: val_accuracy did not improve from 0.90455\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1996 - accuracy: 0.9088 - val_loss: 0.2112 - val_accuracy: 0.9040\n",
            "Epoch 13/50\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1955 - accuracy: 0.9130\n",
            "Epoch 13: val_accuracy improved from 0.90455 to 0.90960, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1967 - accuracy: 0.9121 - val_loss: 0.2047 - val_accuracy: 0.9096\n",
            "Epoch 14/50\n",
            "230/248 [==========================>...] - ETA: 0s - loss: 0.1924 - accuracy: 0.9132\n",
            "Epoch 14: val_accuracy did not improve from 0.90960\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1953 - accuracy: 0.9108 - val_loss: 0.2715 - val_accuracy: 0.8808\n",
            "Epoch 15/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9123\n",
            "Epoch 15: val_accuracy improved from 0.90960 to 0.91111, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.1930 - accuracy: 0.9125 - val_loss: 0.2069 - val_accuracy: 0.9111\n",
            "Epoch 16/50\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1822 - accuracy: 0.9173\n",
            "Epoch 16: val_accuracy did not improve from 0.91111\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.1828 - accuracy: 0.9170 - val_loss: 0.1905 - val_accuracy: 0.9091\n",
            "Epoch 17/50\n",
            "235/248 [===========================>..] - ETA: 0s - loss: 0.1874 - accuracy: 0.9174\n",
            "Epoch 17: val_accuracy did not improve from 0.91111\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1871 - accuracy: 0.9172 - val_loss: 0.2397 - val_accuracy: 0.8909\n",
            "Epoch 18/50\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.1890 - accuracy: 0.9162\n",
            "Epoch 18: val_accuracy did not improve from 0.91111\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1887 - accuracy: 0.9165 - val_loss: 0.2094 - val_accuracy: 0.9045\n",
            "Epoch 19/50\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1818 - accuracy: 0.9202\n",
            "Epoch 19: val_accuracy did not improve from 0.91111\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1802 - accuracy: 0.9207 - val_loss: 0.2007 - val_accuracy: 0.9076\n",
            "Epoch 20/50\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1837 - accuracy: 0.9189\n",
            "Epoch 20: val_accuracy improved from 0.91111 to 0.91818, saving model to best_model.h5\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1821 - accuracy: 0.9190 - val_loss: 0.1895 - val_accuracy: 0.9182\n",
            "Epoch 21/50\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.1803 - accuracy: 0.9171\n",
            "Epoch 21: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1788 - accuracy: 0.9182 - val_loss: 0.1992 - val_accuracy: 0.9136\n",
            "Epoch 22/50\n",
            "233/248 [===========================>..] - ETA: 0s - loss: 0.1726 - accuracy: 0.9211\n",
            "Epoch 22: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1718 - accuracy: 0.9212 - val_loss: 0.1986 - val_accuracy: 0.9121\n",
            "Epoch 23/50\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1712 - accuracy: 0.9211\n",
            "Epoch 23: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1706 - accuracy: 0.9217 - val_loss: 0.2121 - val_accuracy: 0.9051\n",
            "Epoch 24/50\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1710 - accuracy: 0.9206\n",
            "Epoch 24: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1727 - accuracy: 0.9199 - val_loss: 0.2011 - val_accuracy: 0.9111\n",
            "Epoch 25/50\n",
            "232/248 [===========================>..] - ETA: 0s - loss: 0.1784 - accuracy: 0.9208\n",
            "Epoch 25: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1777 - accuracy: 0.9214 - val_loss: 0.1950 - val_accuracy: 0.9121\n",
            "Epoch 26/50\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1698 - accuracy: 0.9220\n",
            "Epoch 26: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1714 - accuracy: 0.9213 - val_loss: 0.1873 - val_accuracy: 0.9152\n",
            "Epoch 27/50\n",
            "232/248 [===========================>..] - ETA: 0s - loss: 0.1762 - accuracy: 0.9232\n",
            "Epoch 27: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1775 - accuracy: 0.9225 - val_loss: 0.2608 - val_accuracy: 0.8864\n",
            "Epoch 28/50\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1678 - accuracy: 0.9239\n",
            "Epoch 28: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1676 - accuracy: 0.9240 - val_loss: 0.2097 - val_accuracy: 0.9091\n",
            "Epoch 29/50\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1689 - accuracy: 0.9237\n",
            "Epoch 29: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1696 - accuracy: 0.9232 - val_loss: 0.1865 - val_accuracy: 0.9136\n",
            "Epoch 30/50\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1657 - accuracy: 0.9234\n",
            "Epoch 30: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1643 - accuracy: 0.9240 - val_loss: 0.2140 - val_accuracy: 0.9091\n",
            "Epoch 31/50\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1668 - accuracy: 0.9241\n",
            "Epoch 31: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.1659 - accuracy: 0.9246 - val_loss: 0.2151 - val_accuracy: 0.9035\n",
            "Epoch 32/50\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1741 - accuracy: 0.9229\n",
            "Epoch 32: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1729 - accuracy: 0.9235 - val_loss: 0.1923 - val_accuracy: 0.9141\n",
            "Epoch 33/50\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1627 - accuracy: 0.9256\n",
            "Epoch 33: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1629 - accuracy: 0.9252 - val_loss: 0.2319 - val_accuracy: 0.8960\n",
            "Epoch 34/50\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1636 - accuracy: 0.9259\n",
            "Epoch 34: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1633 - accuracy: 0.9260 - val_loss: 0.2005 - val_accuracy: 0.9141\n",
            "Epoch 35/50\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9285\n",
            "Epoch 35: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1622 - accuracy: 0.9284 - val_loss: 0.1853 - val_accuracy: 0.9146\n",
            "Epoch 36/50\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1640 - accuracy: 0.9256\n",
            "Epoch 36: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1651 - accuracy: 0.9246 - val_loss: 0.2107 - val_accuracy: 0.9081\n",
            "Epoch 37/50\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1638 - accuracy: 0.9253\n",
            "Epoch 37: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1642 - accuracy: 0.9249 - val_loss: 0.2042 - val_accuracy: 0.9081\n",
            "Epoch 38/50\n",
            "229/248 [==========================>...] - ETA: 0s - loss: 0.1632 - accuracy: 0.9281\n",
            "Epoch 38: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1617 - accuracy: 0.9285 - val_loss: 0.1995 - val_accuracy: 0.9131\n",
            "Epoch 39/50\n",
            "227/248 [==========================>...] - ETA: 0s - loss: 0.1598 - accuracy: 0.9279\n",
            "Epoch 39: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1590 - accuracy: 0.9283 - val_loss: 0.2022 - val_accuracy: 0.9101\n",
            "Epoch 40/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1634 - accuracy: 0.9284\n",
            "Epoch 40: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1633 - accuracy: 0.9285 - val_loss: 0.2065 - val_accuracy: 0.9096\n",
            "Epoch 41/50\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1602 - accuracy: 0.9297\n",
            "Epoch 41: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1606 - accuracy: 0.9297 - val_loss: 0.2072 - val_accuracy: 0.9106\n",
            "Epoch 42/50\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9291\n",
            "Epoch 42: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1539 - accuracy: 0.9291 - val_loss: 0.2196 - val_accuracy: 0.9056\n",
            "Epoch 43/50\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1579 - accuracy: 0.9280\n",
            "Epoch 43: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1570 - accuracy: 0.9285 - val_loss: 0.2122 - val_accuracy: 0.9086\n",
            "Epoch 44/50\n",
            "231/248 [==========================>...] - ETA: 0s - loss: 0.1585 - accuracy: 0.9307\n",
            "Epoch 44: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1568 - accuracy: 0.9319 - val_loss: 0.1912 - val_accuracy: 0.9162\n",
            "Epoch 45/50\n",
            "230/248 [==========================>...] - ETA: 0s - loss: 0.1574 - accuracy: 0.9279\n",
            "Epoch 45: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1552 - accuracy: 0.9295 - val_loss: 0.2113 - val_accuracy: 0.9056\n",
            "Epoch 46/50\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9285\n",
            "Epoch 46: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1540 - accuracy: 0.9286 - val_loss: 0.2039 - val_accuracy: 0.9141\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9347\n",
            "Epoch 47: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1520 - accuracy: 0.9347 - val_loss: 0.1990 - val_accuracy: 0.9141\n",
            "Epoch 48/50\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1536 - accuracy: 0.9294\n",
            "Epoch 48: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.1536 - accuracy: 0.9298 - val_loss: 0.1932 - val_accuracy: 0.9152\n",
            "Epoch 49/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1491 - accuracy: 0.9335\n",
            "Epoch 49: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.1496 - accuracy: 0.9332 - val_loss: 0.2027 - val_accuracy: 0.9086\n",
            "Epoch 50/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1505 - accuracy: 0.9331\n",
            "Epoch 50: val_accuracy did not improve from 0.91818\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1503 - accuracy: 0.9332 - val_loss: 0.2013 - val_accuracy: 0.9121\n",
            "310/310 [==============================] - 1s 2ms/step - loss: 0.1630 - accuracy: 0.9275\n",
            "Train accuracy: 0.9275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Shuffle data\n",
        "shuffled_data = normalized_data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Split data, test and validation\n",
        "train_size = int(0.8 * len(shuffled_data))\n",
        "X_train, X_val = shuffled_data.iloc[:train_size, :-1], shuffled_data.iloc[train_size:, :-1]\n",
        "y_train, y_val = shuffled_data.iloc[:train_size, -1], shuffled_data.iloc[train_size:, -1]\n",
        "\n",
        "# Train a deep model with a larger number of neurons for validation dataset\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_val = ModelCheckpoint(\"best_model_val.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "checkpoint_train = ModelCheckpoint(\"best_model_train.h5\", monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "history_val = model.fit(X_train, y_train, epochs=200, validation_data=(X_val, y_val), callbacks=[checkpoint_val, checkpoint_train])\n",
        "\n",
        "# Print accuracy on both training and validation dataset\n",
        "best_model_val = tf.keras.models.load_model(\"best_model_val.h5\")\n",
        "best_model_train = tf.keras.models.load_model(\"best_model_train.h5\")\n",
        "_, train_accuracy = best_model_train.evaluate(X_train, y_train, verbose=1)\n",
        "_, val_accuracy = best_model_val.evaluate(X_val, y_val, verbose=1)\n",
        "print(f\"Train accuracy: {train_accuracy:.4f}, Validation accuracy: {val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi5GoXhbRdEe",
        "outputId": "2a19cc18-d525-45ef-a955-0266b9115b7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.5222 - accuracy: 0.7197\n",
            "Epoch 1: val_accuracy improved from -inf to 0.86364, saving model to best_model_val.h5\n",
            "\n",
            "Epoch 1: accuracy improved from -inf to 0.72127, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 3s 6ms/step - loss: 0.5202 - accuracy: 0.7213 - val_loss: 0.3304 - val_accuracy: 0.8636\n",
            "Epoch 2/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.8643\n",
            "Epoch 2: val_accuracy did not improve from 0.86364\n",
            "\n",
            "Epoch 2: accuracy improved from 0.72127 to 0.86499, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 8ms/step - loss: 0.3024 - accuracy: 0.8650 - val_loss: 0.2908 - val_accuracy: 0.8586\n",
            "Epoch 3/50\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.2607 - accuracy: 0.8877\n",
            "Epoch 3: val_accuracy improved from 0.86364 to 0.86768, saving model to best_model_val.h5\n",
            "\n",
            "Epoch 3: accuracy improved from 0.86499 to 0.88760, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 8ms/step - loss: 0.2605 - accuracy: 0.8876 - val_loss: 0.2872 - val_accuracy: 0.8677\n",
            "Epoch 4/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.8906\n",
            "Epoch 4: val_accuracy improved from 0.86768 to 0.90152, saving model to best_model_val.h5\n",
            "\n",
            "Epoch 4: accuracy improved from 0.88760 to 0.89151, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.2519 - accuracy: 0.8915 - val_loss: 0.2375 - val_accuracy: 0.9015\n",
            "Epoch 5/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.2398 - accuracy: 0.8955\n",
            "Epoch 5: val_accuracy did not improve from 0.90152\n",
            "\n",
            "Epoch 5: accuracy improved from 0.89151 to 0.89454, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 6ms/step - loss: 0.2409 - accuracy: 0.8945 - val_loss: 0.2810 - val_accuracy: 0.8727\n",
            "Epoch 6/50\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.2289 - accuracy: 0.8986\n",
            "Epoch 6: val_accuracy improved from 0.90152 to 0.91364, saving model to best_model_val.h5\n",
            "\n",
            "Epoch 6: accuracy improved from 0.89454 to 0.89821, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 7ms/step - loss: 0.2286 - accuracy: 0.8982 - val_loss: 0.2089 - val_accuracy: 0.9136\n",
            "Epoch 7/50\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.2214 - accuracy: 0.9074\n",
            "Epoch 7: val_accuracy did not improve from 0.91364\n",
            "\n",
            "Epoch 7: accuracy improved from 0.89821 to 0.90793, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 6ms/step - loss: 0.2201 - accuracy: 0.9079 - val_loss: 0.2159 - val_accuracy: 0.8990\n",
            "Epoch 8/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.2053 - accuracy: 0.9117\n",
            "Epoch 8: val_accuracy improved from 0.91364 to 0.91717, saving model to best_model_val.h5\n",
            "\n",
            "Epoch 8: accuracy improved from 0.90793 to 0.91185, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 6ms/step - loss: 0.2043 - accuracy: 0.9118 - val_loss: 0.1975 - val_accuracy: 0.9172\n",
            "Epoch 9/50\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.9107\n",
            "Epoch 9: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 9: accuracy did not improve from 0.91185\n",
            "248/248 [==============================] - 2s 6ms/step - loss: 0.2008 - accuracy: 0.9107 - val_loss: 0.2378 - val_accuracy: 0.8970\n",
            "Epoch 10/50\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.2028 - accuracy: 0.9119\n",
            "Epoch 10: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 10: accuracy improved from 0.91185 to 0.91235, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 3s 12ms/step - loss: 0.2023 - accuracy: 0.9124 - val_loss: 0.2369 - val_accuracy: 0.8955\n",
            "Epoch 11/50\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1955 - accuracy: 0.9146\n",
            "Epoch 11: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 11: accuracy improved from 0.91235 to 0.91361, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 8ms/step - loss: 0.1976 - accuracy: 0.9136 - val_loss: 0.2093 - val_accuracy: 0.9071\n",
            "Epoch 12/50\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9142\n",
            "Epoch 12: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 12: accuracy improved from 0.91361 to 0.91374, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 7ms/step - loss: 0.2010 - accuracy: 0.9137 - val_loss: 0.2073 - val_accuracy: 0.9051\n",
            "Epoch 13/50\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9196\n",
            "Epoch 13: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 13: accuracy improved from 0.91374 to 0.91930, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 6ms/step - loss: 0.1824 - accuracy: 0.9193 - val_loss: 0.2035 - val_accuracy: 0.9051\n",
            "Epoch 14/50\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1895 - accuracy: 0.9124\n",
            "Epoch 14: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 14: accuracy did not improve from 0.91930\n",
            "248/248 [==============================] - 2s 7ms/step - loss: 0.1894 - accuracy: 0.9126 - val_loss: 0.2053 - val_accuracy: 0.9071\n",
            "Epoch 15/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1833 - accuracy: 0.9183\n",
            "Epoch 15: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 15: accuracy did not improve from 0.91930\n",
            "248/248 [==============================] - 2s 7ms/step - loss: 0.1827 - accuracy: 0.9183 - val_loss: 0.2166 - val_accuracy: 0.9000\n",
            "Epoch 16/50\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1785 - accuracy: 0.9225\n",
            "Epoch 16: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 16: accuracy improved from 0.91930 to 0.92296, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 7ms/step - loss: 0.1780 - accuracy: 0.9230 - val_loss: 0.1917 - val_accuracy: 0.9131\n",
            "Epoch 17/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1729 - accuracy: 0.9265\n",
            "Epoch 17: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 17: accuracy improved from 0.92296 to 0.92637, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 8ms/step - loss: 0.1731 - accuracy: 0.9264 - val_loss: 0.2116 - val_accuracy: 0.9106\n",
            "Epoch 18/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9163\n",
            "Epoch 18: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 18: accuracy did not improve from 0.92637\n",
            "248/248 [==============================] - 2s 7ms/step - loss: 0.1879 - accuracy: 0.9160 - val_loss: 0.1890 - val_accuracy: 0.9131\n",
            "Epoch 19/50\n",
            "237/248 [===========================>..] - ETA: 0s - loss: 0.1712 - accuracy: 0.9225\n",
            "Epoch 19: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 19: accuracy did not improve from 0.92637\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.1721 - accuracy: 0.9221 - val_loss: 0.2282 - val_accuracy: 0.8934\n",
            "Epoch 20/50\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9204\n",
            "Epoch 20: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 20: accuracy did not improve from 0.92637\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.1712 - accuracy: 0.9213 - val_loss: 0.2388 - val_accuracy: 0.9040\n",
            "Epoch 21/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9240\n",
            "Epoch 21: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 21: accuracy did not improve from 0.92637\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.1742 - accuracy: 0.9241 - val_loss: 0.1984 - val_accuracy: 0.9076\n",
            "Epoch 22/50\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1842 - accuracy: 0.9177\n",
            "Epoch 22: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 22: accuracy did not improve from 0.92637\n",
            "248/248 [==============================] - 3s 11ms/step - loss: 0.1841 - accuracy: 0.9179 - val_loss: 0.1930 - val_accuracy: 0.9106\n",
            "Epoch 23/50\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1724 - accuracy: 0.9223\n",
            "Epoch 23: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 23: accuracy did not improve from 0.92637\n",
            "248/248 [==============================] - 3s 13ms/step - loss: 0.1724 - accuracy: 0.9223 - val_loss: 0.2329 - val_accuracy: 0.8980\n",
            "Epoch 24/50\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1753 - accuracy: 0.9217\n",
            "Epoch 24: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 24: accuracy did not improve from 0.92637\n",
            "248/248 [==============================] - 3s 11ms/step - loss: 0.1756 - accuracy: 0.9218 - val_loss: 0.1863 - val_accuracy: 0.9136\n",
            "Epoch 25/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9293\n",
            "Epoch 25: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 25: accuracy improved from 0.92637 to 0.92940, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 6ms/step - loss: 0.1610 - accuracy: 0.9294 - val_loss: 0.1964 - val_accuracy: 0.9146\n",
            "Epoch 26/50\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1834 - accuracy: 0.9205\n",
            "Epoch 26: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 26: accuracy did not improve from 0.92940\n",
            "248/248 [==============================] - 3s 10ms/step - loss: 0.1836 - accuracy: 0.9204 - val_loss: 0.1933 - val_accuracy: 0.9162\n",
            "Epoch 27/50\n",
            "245/248 [============================>.] - ETA: 0s - loss: 0.1657 - accuracy: 0.9254\n",
            "Epoch 27: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 27: accuracy did not improve from 0.92940\n",
            "248/248 [==============================] - 3s 10ms/step - loss: 0.1647 - accuracy: 0.9259 - val_loss: 0.1997 - val_accuracy: 0.9076\n",
            "Epoch 28/50\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9268\n",
            "Epoch 28: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 28: accuracy did not improve from 0.92940\n",
            "248/248 [==============================] - 2s 8ms/step - loss: 0.1599 - accuracy: 0.9275 - val_loss: 0.2374 - val_accuracy: 0.8965\n",
            "Epoch 29/50\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1637 - accuracy: 0.9264\n",
            "Epoch 29: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 29: accuracy did not improve from 0.92940\n",
            "248/248 [==============================] - 2s 10ms/step - loss: 0.1639 - accuracy: 0.9262 - val_loss: 0.2032 - val_accuracy: 0.9056\n",
            "Epoch 30/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1606 - accuracy: 0.9278\n",
            "Epoch 30: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 30: accuracy did not improve from 0.92940\n",
            "248/248 [==============================] - 4s 16ms/step - loss: 0.1607 - accuracy: 0.9276 - val_loss: 0.1976 - val_accuracy: 0.9101\n",
            "Epoch 31/50\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1616 - accuracy: 0.9273\n",
            "Epoch 31: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 31: accuracy did not improve from 0.92940\n",
            "248/248 [==============================] - 2s 9ms/step - loss: 0.1616 - accuracy: 0.9273 - val_loss: 0.1921 - val_accuracy: 0.9157\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - ETA: 0s - loss: 0.1640 - accuracy: 0.9267\n",
            "Epoch 32: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 32: accuracy did not improve from 0.92940\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1640 - accuracy: 0.9267 - val_loss: 0.2090 - val_accuracy: 0.8990\n",
            "Epoch 33/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9300\n",
            "Epoch 33: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 33: accuracy improved from 0.92940 to 0.93054, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1602 - accuracy: 0.9305 - val_loss: 0.2462 - val_accuracy: 0.9015\n",
            "Epoch 34/50\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1596 - accuracy: 0.9272\n",
            "Epoch 34: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 34: accuracy did not improve from 0.93054\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1597 - accuracy: 0.9276 - val_loss: 0.1938 - val_accuracy: 0.9136\n",
            "Epoch 35/50\n",
            "239/248 [===========================>..] - ETA: 0s - loss: 0.1551 - accuracy: 0.9293\n",
            "Epoch 35: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 35: accuracy did not improve from 0.93054\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1560 - accuracy: 0.9293 - val_loss: 0.2111 - val_accuracy: 0.9040\n",
            "Epoch 36/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9329\n",
            "Epoch 36: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 36: accuracy improved from 0.93054 to 0.93294, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 2s 7ms/step - loss: 0.1547 - accuracy: 0.9329 - val_loss: 0.1988 - val_accuracy: 0.9086\n",
            "Epoch 37/50\n",
            "243/248 [============================>.] - ETA: 0s - loss: 0.1549 - accuracy: 0.9304\n",
            "Epoch 37: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 37: accuracy did not improve from 0.93294\n",
            "248/248 [==============================] - 2s 9ms/step - loss: 0.1552 - accuracy: 0.9304 - val_loss: 0.2135 - val_accuracy: 0.9030\n",
            "Epoch 38/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1467 - accuracy: 0.9319\n",
            "Epoch 38: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 38: accuracy did not improve from 0.93294\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1466 - accuracy: 0.9323 - val_loss: 0.2155 - val_accuracy: 0.9035\n",
            "Epoch 39/50\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9339\n",
            "Epoch 39: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 39: accuracy improved from 0.93294 to 0.93370, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1515 - accuracy: 0.9337 - val_loss: 0.2097 - val_accuracy: 0.9056\n",
            "Epoch 40/50\n",
            "246/248 [============================>.] - ETA: 0s - loss: 0.1441 - accuracy: 0.9352\n",
            "Epoch 40: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 40: accuracy improved from 0.93370 to 0.93496, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1448 - accuracy: 0.9350 - val_loss: 0.2167 - val_accuracy: 0.9071\n",
            "Epoch 41/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.9343\n",
            "Epoch 41: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 41: accuracy did not improve from 0.93496\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1455 - accuracy: 0.9345 - val_loss: 0.1995 - val_accuracy: 0.9071\n",
            "Epoch 42/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9332\n",
            "Epoch 42: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 42: accuracy did not improve from 0.93496\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1513 - accuracy: 0.9331 - val_loss: 0.2125 - val_accuracy: 0.9040\n",
            "Epoch 43/50\n",
            "238/248 [===========================>..] - ETA: 0s - loss: 0.1410 - accuracy: 0.9389\n",
            "Epoch 43: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 43: accuracy improved from 0.93496 to 0.93837, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1419 - accuracy: 0.9384 - val_loss: 0.2274 - val_accuracy: 0.9045\n",
            "Epoch 44/50\n",
            "247/248 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9319\n",
            "Epoch 44: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 44: accuracy did not improve from 0.93837\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1445 - accuracy: 0.9321 - val_loss: 0.2152 - val_accuracy: 0.9040\n",
            "Epoch 45/50\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.9347\n",
            "Epoch 45: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 45: accuracy did not improve from 0.93837\n",
            "248/248 [==============================] - 2s 8ms/step - loss: 0.1481 - accuracy: 0.9341 - val_loss: 0.2242 - val_accuracy: 0.9035\n",
            "Epoch 46/50\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.9374\n",
            "Epoch 46: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 46: accuracy did not improve from 0.93837\n",
            "248/248 [==============================] - 2s 8ms/step - loss: 0.1435 - accuracy: 0.9380 - val_loss: 0.2628 - val_accuracy: 0.8919\n",
            "Epoch 47/50\n",
            "240/248 [============================>.] - ETA: 0s - loss: 0.1437 - accuracy: 0.9366\n",
            "Epoch 47: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 47: accuracy did not improve from 0.93837\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1449 - accuracy: 0.9361 - val_loss: 0.2306 - val_accuracy: 0.8955\n",
            "Epoch 48/50\n",
            "242/248 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.9358\n",
            "Epoch 48: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 48: accuracy did not improve from 0.93837\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.1447 - accuracy: 0.9362 - val_loss: 0.2191 - val_accuracy: 0.9121\n",
            "Epoch 49/50\n",
            "244/248 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.9387\n",
            "Epoch 49: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 49: accuracy improved from 0.93837 to 0.93849, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1440 - accuracy: 0.9385 - val_loss: 0.2184 - val_accuracy: 0.8955\n",
            "Epoch 50/50\n",
            "241/248 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9420\n",
            "Epoch 50: val_accuracy did not improve from 0.91717\n",
            "\n",
            "Epoch 50: accuracy improved from 0.93849 to 0.94190, saving model to best_model_train.h5\n",
            "248/248 [==============================] - 1s 6ms/step - loss: 0.1331 - accuracy: 0.9419 - val_loss: 0.2346 - val_accuracy: 0.9020\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.1317 - accuracy: 0.9406\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9172\n",
            "Train accuracy: 0.9406, Validation accuracy: 0.9172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split the data into features (X) and target (y)\n",
        "X = normalized_data.drop(columns=[\"brand\"])\n",
        "y = normalized_data[\"brand\"]\n",
        "\n",
        "# Get number of features\n",
        "n_features = X.shape[1]\n",
        "\n",
        "# Train models with single features\n",
        "single_feature_accuracies = []\n",
        "for i in range(n_features):\n",
        "    # Select single feature\n",
        "    X_single = X.iloc[:, i:i+1]\n",
        "    # Split data into training and validation sets\n",
        "    train_size = int(0.8 * len(X_single))\n",
        "    X_train, X_val = X_single.iloc[:train_size], X_single.iloc[train_size:]\n",
        "    y_train, y_val = y.iloc[:train_size], y.iloc[train_size:]\n",
        "    # Build and train model\n",
        "    model = Sequential([\n",
        "        Dense(32, activation=\"relu\", input_shape=[1]),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    # Define checkpoint to save best model\n",
        "    checkpoint = ModelCheckpoint(\"best_single_feature_model.h5\", save_best_only=True, save_weights_only=True)\n",
        "    # Train model with checkpointing\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, callbacks=[checkpoint], verbose=1)\n",
        "    # Load best model and compute validation accuracy\n",
        "    model.load_weights(\"best_single_feature_model.h5\")\n",
        "    _, accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
        "    single_feature_accuracies.append(accuracy)\n",
        "\n",
        "# Plot single-feature accuracies\n",
        "plt.bar(range(n_features), single_feature_accuracies)\n",
        "plt.xlabel(\"Feature Index\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracies of Models with Single Features\")\n",
        "plt.show()\n",
        "\n",
        "# Train models with reduced feature sets\n",
        "reduced_feature_accuracies = [max(single_feature_accuracies)]\n",
        "for i in range(1, n_features):\n",
        "    # Select features to keep\n",
        "    features_to_keep = np.argsort(single_feature_accuracies)[:n_features-i]\n",
        "    X_reduced = X.iloc[:, features_to_keep]\n",
        "    # Split data into training and validation sets\n",
        "    train_size = int(0.8 * len(X_reduced))\n",
        "    X_train, X_val = X_reduced.iloc[:train_size], X_reduced.iloc[train_size:]\n",
        "    y_train, y_val = y.iloc[:train_size], y.iloc[train_size:]\n",
        "    # Build and train model\n",
        "    model = Sequential([\n",
        "        Dense(32, activation=\"relu\", input_shape=[X_reduced.shape[1]]),\n",
        "        Dense(64, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    # Define checkpoint to save best model\n",
        "    checkpoint = ModelCheckpoint(\"best_reduced_feature_model.h5\", save_best_only=True, save_weights_only=True)\n",
        "    # Train model with checkpointing\n",
        "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, callbacks=[checkpoint], verbose=1)\n",
        "    # Load best model and compute validation accuracy\n",
        "    model.load_weights(\"best_reduced_feature_model.h5\")\n",
        "    _, accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
        "    reduced_feature_accuracies.append(accuracy)\n",
        "# Plot accuracies of models with reduced feature sets\n",
        "plt.plot(range(len(reduced_feature_accuracies)), reduced_feature_accuracies)\n",
        "plt.xlabel(\"Number of Features Removed\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracies of Models with Reduced Feature Sets\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vcSCtyI1RmPs",
        "outputId": "bb1c0297-e800-496c-d1d5-482872ef0891"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 3ms/step - loss: 0.6356 - accuracy: 0.6192 - val_loss: 0.5977 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5745 - accuracy: 0.6711 - val_loss: 0.5380 - val_accuracy: 0.7141\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5367 - accuracy: 0.7240 - val_loss: 0.5214 - val_accuracy: 0.7268\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5294 - accuracy: 0.7315 - val_loss: 0.5147 - val_accuracy: 0.7338\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5274 - accuracy: 0.7285 - val_loss: 0.5134 - val_accuracy: 0.7328\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.5273 - accuracy: 0.7305 - val_loss: 0.5168 - val_accuracy: 0.7303\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7294 - val_loss: 0.5134 - val_accuracy: 0.7338\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.5254 - accuracy: 0.7305 - val_loss: 0.5127 - val_accuracy: 0.7338\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7267 - val_loss: 0.5133 - val_accuracy: 0.7364\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5244 - accuracy: 0.7294 - val_loss: 0.5129 - val_accuracy: 0.7354\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7301 - val_loss: 0.5115 - val_accuracy: 0.7348\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5234 - accuracy: 0.7318 - val_loss: 0.5142 - val_accuracy: 0.7298\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5238 - accuracy: 0.7294 - val_loss: 0.5108 - val_accuracy: 0.7384\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7296 - val_loss: 0.5124 - val_accuracy: 0.7283\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5235 - accuracy: 0.7281 - val_loss: 0.5104 - val_accuracy: 0.7384\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5228 - accuracy: 0.7290 - val_loss: 0.5099 - val_accuracy: 0.7328\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5223 - accuracy: 0.7292 - val_loss: 0.5091 - val_accuracy: 0.7364\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5222 - accuracy: 0.7302 - val_loss: 0.5099 - val_accuracy: 0.7359\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7290 - val_loss: 0.5103 - val_accuracy: 0.7333\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5218 - accuracy: 0.7297 - val_loss: 0.5093 - val_accuracy: 0.7318\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7307 - val_loss: 0.5100 - val_accuracy: 0.7333\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5214 - accuracy: 0.7286 - val_loss: 0.5089 - val_accuracy: 0.7389\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7290 - val_loss: 0.5099 - val_accuracy: 0.7298\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7299 - val_loss: 0.5090 - val_accuracy: 0.7318\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.5197 - accuracy: 0.7312 - val_loss: 0.5105 - val_accuracy: 0.7293\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.5202 - accuracy: 0.7270 - val_loss: 0.5094 - val_accuracy: 0.7338\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.5192 - accuracy: 0.7288 - val_loss: 0.5077 - val_accuracy: 0.7343\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7296 - val_loss: 0.5100 - val_accuracy: 0.7338\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7299 - val_loss: 0.5093 - val_accuracy: 0.7283\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7320 - val_loss: 0.5062 - val_accuracy: 0.7359\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7299 - val_loss: 0.5058 - val_accuracy: 0.7374\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7311 - val_loss: 0.5055 - val_accuracy: 0.7369\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7307 - val_loss: 0.5090 - val_accuracy: 0.7263\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5189 - accuracy: 0.7299 - val_loss: 0.5061 - val_accuracy: 0.7338\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7311 - val_loss: 0.5131 - val_accuracy: 0.7333\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7318 - val_loss: 0.5066 - val_accuracy: 0.7323\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7300 - val_loss: 0.5075 - val_accuracy: 0.7364\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7287 - val_loss: 0.5064 - val_accuracy: 0.7348\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7281 - val_loss: 0.5067 - val_accuracy: 0.7374\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5180 - accuracy: 0.7309 - val_loss: 0.5062 - val_accuracy: 0.7364\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7299 - val_loss: 0.5058 - val_accuracy: 0.7348\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7309 - val_loss: 0.5054 - val_accuracy: 0.7328\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7330 - val_loss: 0.5056 - val_accuracy: 0.7338\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.5174 - accuracy: 0.7295 - val_loss: 0.5073 - val_accuracy: 0.7308\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.5184 - accuracy: 0.7290 - val_loss: 0.5052 - val_accuracy: 0.7369\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.5175 - accuracy: 0.7318 - val_loss: 0.5049 - val_accuracy: 0.7374\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7314 - val_loss: 0.5120 - val_accuracy: 0.7343\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5176 - accuracy: 0.7310 - val_loss: 0.5111 - val_accuracy: 0.7308\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7296 - val_loss: 0.5070 - val_accuracy: 0.7343\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.5176 - accuracy: 0.7305 - val_loss: 0.5052 - val_accuracy: 0.7348\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7374\n",
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 4ms/step - loss: 0.6669 - accuracy: 0.6168 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6193 - val_loss: 0.6578 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6608 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6602 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6601 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.6578 - accuracy: 0.6313\n",
            "Epoch 1/50\n",
            "248/248 [==============================] - 3s 9ms/step - loss: 0.6681 - accuracy: 0.6183 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6193 - val_loss: 0.6608 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6652 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6601 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6599 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6599 - val_accuracy: 0.6313\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.6313\n",
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 3ms/step - loss: 0.6678 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6656 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6599 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6601 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6601 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6599 - val_accuracy: 0.6313\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6579 - accuracy: 0.6313\n",
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 4ms/step - loss: 0.6686 - accuracy: 0.6163 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6651 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6596 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6596 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 2s 7ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6580 - accuracy: 0.6313\n",
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 4ms/step - loss: 0.6685 - accuracy: 0.6162 - val_loss: 0.6612 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6656 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.6193 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6599 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6605 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6585 - accuracy: 0.6313\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 3ms/step - loss: 0.6677 - accuracy: 0.6164 - val_loss: 0.6616 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6653 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6578 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6574 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.6193 - val_loss: 0.6573 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6576 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6641 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6638 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6636 - accuracy: 0.6193 - val_loss: 0.6578 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6635 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6637 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6631 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6629 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.6193 - val_loss: 0.6617 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6629 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6625 - accuracy: 0.6193 - val_loss: 0.6611 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6193 - val_loss: 0.6606 - val_accuracy: 0.6318\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6618 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6195 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6195 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6195 - val_loss: 0.6605 - val_accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6609 - accuracy: 0.6192 - val_loss: 0.6609 - val_accuracy: 0.6318\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6613 - accuracy: 0.6196 - val_loss: 0.6614 - val_accuracy: 0.6318\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6199 - val_loss: 0.6601 - val_accuracy: 0.6323\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6608 - accuracy: 0.6191 - val_loss: 0.6603 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6601 - accuracy: 0.6201 - val_loss: 0.6598 - val_accuracy: 0.6318\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6599 - accuracy: 0.6210 - val_loss: 0.6601 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6202 - val_loss: 0.6616 - val_accuracy: 0.6323\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6199 - val_loss: 0.6613 - val_accuracy: 0.6303\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6207 - val_loss: 0.6624 - val_accuracy: 0.6303\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6210 - val_loss: 0.6625 - val_accuracy: 0.6273\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6593 - accuracy: 0.6204 - val_loss: 0.6618 - val_accuracy: 0.6293\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6592 - accuracy: 0.6212 - val_loss: 0.6623 - val_accuracy: 0.6278\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6587 - accuracy: 0.6186 - val_loss: 0.6636 - val_accuracy: 0.6278\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6200 - val_loss: 0.6615 - val_accuracy: 0.6313\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6586 - accuracy: 0.6199 - val_loss: 0.6628 - val_accuracy: 0.6288\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6211 - val_loss: 0.6624 - val_accuracy: 0.6283\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6210 - val_loss: 0.6641 - val_accuracy: 0.6288\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6580 - accuracy: 0.6216 - val_loss: 0.6654 - val_accuracy: 0.6222\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6579 - accuracy: 0.6207 - val_loss: 0.6653 - val_accuracy: 0.6217\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6221 - val_loss: 0.6634 - val_accuracy: 0.6323\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6573 - accuracy: 0.6217 - val_loss: 0.6638 - val_accuracy: 0.6232\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6574 - accuracy: 0.6199 - val_loss: 0.6642 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6221 - val_loss: 0.6640 - val_accuracy: 0.6247\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6570 - accuracy: 0.6221 - val_loss: 0.6652 - val_accuracy: 0.6323\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6313\n",
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 5ms/step - loss: 0.6671 - accuracy: 0.6190 - val_loss: 0.6573 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6193 - val_loss: 0.6574 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6571 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6577 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6575 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6574 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6574 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6577 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.6193 - val_loss: 0.6602 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6637 - accuracy: 0.6193 - val_loss: 0.6578 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.6193 - val_loss: 0.6576 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6636 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6633 - accuracy: 0.6193 - val_loss: 0.6576 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6635 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6631 - accuracy: 0.6193 - val_loss: 0.6578 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6628 - accuracy: 0.6195 - val_loss: 0.6596 - val_accuracy: 0.6303\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6631 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6192 - val_loss: 0.6622 - val_accuracy: 0.6303\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6192 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6196 - val_loss: 0.6609 - val_accuracy: 0.6303\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6192 - val_loss: 0.6598 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6193 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6622 - accuracy: 0.6190 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6190 - val_loss: 0.6602 - val_accuracy: 0.6313\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6622 - accuracy: 0.6196 - val_loss: 0.6607 - val_accuracy: 0.6303\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6195 - val_loss: 0.6590 - val_accuracy: 0.6303\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6193 - val_loss: 0.6618 - val_accuracy: 0.6303\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6197 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6193 - val_loss: 0.6607 - val_accuracy: 0.6303\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6619 - accuracy: 0.6195 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6191 - val_loss: 0.6589 - val_accuracy: 0.6303\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6618 - accuracy: 0.6195 - val_loss: 0.6604 - val_accuracy: 0.6303\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6199 - val_loss: 0.6621 - val_accuracy: 0.6308\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6193 - val_loss: 0.6596 - val_accuracy: 0.6313\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6612 - accuracy: 0.6193 - val_loss: 0.6607 - val_accuracy: 0.6308\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6193 - val_loss: 0.6596 - val_accuracy: 0.6313\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6615 - accuracy: 0.6190 - val_loss: 0.6607 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6197 - val_loss: 0.6599 - val_accuracy: 0.6313\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6196 - val_loss: 0.6618 - val_accuracy: 0.6288\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6571 - accuracy: 0.6313\n",
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 3ms/step - loss: 0.6671 - accuracy: 0.6185 - val_loss: 0.6573 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6658 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6607 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6578 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6612 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6605 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6596 - val_accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6598 - val_accuracy: 0.6313\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6641 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6193 - val_loss: 0.6594 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6639 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6639 - accuracy: 0.6193 - val_loss: 0.6601 - val_accuracy: 0.6313\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6639 - accuracy: 0.6193 - val_loss: 0.6598 - val_accuracy: 0.6313\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6638 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6637 - accuracy: 0.6193 - val_loss: 0.6596 - val_accuracy: 0.6313\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6637 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.6313\n",
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 3ms/step - loss: 0.6668 - accuracy: 0.6172 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6652 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6193 - val_loss: 0.6601 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6600 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6592 - val_accuracy: 0.6313\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6641 - accuracy: 0.6193 - val_loss: 0.6597 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6641 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6641 - accuracy: 0.6193 - val_loss: 0.6598 - val_accuracy: 0.6313\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6580 - accuracy: 0.6313\n",
            "Epoch 1/50\n",
            "248/248 [==============================] - 2s 3ms/step - loss: 0.6675 - accuracy: 0.6193 - val_loss: 0.6601 - val_accuracy: 0.6313\n",
            "Epoch 2/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 3/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6595 - val_accuracy: 0.6313\n",
            "Epoch 4/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 5/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6607 - val_accuracy: 0.6313\n",
            "Epoch 6/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6598 - val_accuracy: 0.6313\n",
            "Epoch 7/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 8/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 9/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6600 - val_accuracy: 0.6313\n",
            "Epoch 10/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 11/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 12/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 13/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 14/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6596 - val_accuracy: 0.6313\n",
            "Epoch 15/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 16/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 17/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 18/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 19/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 20/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 21/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6193 - val_loss: 0.6583 - val_accuracy: 0.6313\n",
            "Epoch 22/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 23/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 24/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6579 - val_accuracy: 0.6313\n",
            "Epoch 25/50\n",
            "248/248 [==============================] - 1s 5ms/step - loss: 0.6648 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 26/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 27/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6591 - val_accuracy: 0.6313\n",
            "Epoch 28/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6580 - val_accuracy: 0.6313\n",
            "Epoch 29/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 30/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 31/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6602 - val_accuracy: 0.6313\n",
            "Epoch 32/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6593 - val_accuracy: 0.6313\n",
            "Epoch 33/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 34/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6584 - val_accuracy: 0.6313\n",
            "Epoch 36/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6588 - val_accuracy: 0.6313\n",
            "Epoch 37/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6600 - val_accuracy: 0.6313\n",
            "Epoch 38/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6581 - val_accuracy: 0.6313\n",
            "Epoch 39/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6586 - val_accuracy: 0.6313\n",
            "Epoch 41/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 42/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6585 - val_accuracy: 0.6313\n",
            "Epoch 43/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6643 - accuracy: 0.6193 - val_loss: 0.6596 - val_accuracy: 0.6313\n",
            "Epoch 44/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 45/50\n",
            "248/248 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6596 - val_accuracy: 0.6313\n",
            "Epoch 46/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 47/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6587 - val_accuracy: 0.6313\n",
            "Epoch 48/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6589 - val_accuracy: 0.6313\n",
            "Epoch 49/50\n",
            "248/248 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6193 - val_loss: 0.6590 - val_accuracy: 0.6313\n",
            "Epoch 50/50\n",
            "248/248 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6193 - val_loss: 0.6582 - val_accuracy: 0.6313\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6579 - accuracy: 0.6313\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}